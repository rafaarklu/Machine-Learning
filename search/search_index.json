{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#machine-learning-documentation-project","title":"Machine Learning documentation Project","text":""},{"location":"#autor","title":"Autor","text":"<p>Author</p> <p>Rafael Arkchimor Lucena</p> <ul> <li> <p>Linkedin: https://www.linkedin.com/in/rafael-lucena-0273b82b3/</p> </li> <li> <p>Github: https://github.com/rafaarklu/</p> </li> <li> <p>email: rafa.arklu@gmail.com</p> </li> </ul> <p>Este projeto re\u00fane material de refer\u00eancia, scripts e exemplos pr\u00e1ticos dos principais m\u00e9todos estudados: K-Nearest Neighbors (KNN), K-Means, \u00c1rvores de Decis\u00e3o, Random Forest e m\u00e9tricas de avalia\u00e7\u00e3o. O objetivo \u00e9 servir como material did\u00e1tico e reposit\u00f3rio de apoio para estudos e experimentos.</p>"},{"location":"#o-que-ha-neste-repositorio","title":"O que h\u00e1 neste reposit\u00f3rio","text":"<p>Uma vis\u00e3o geral das se\u00e7\u00f5es/documentos dispon\u00edveis:</p> <ul> <li>K-Nearest Neighbors (KNN) \u2014 abordagem cl\u00e1ssica de classifica\u00e7\u00e3o; exemplos e c\u00f3digo: ./k-nearest-neighbor/main.md</li> <li>K-Means \u2014 algoritmo de clusteriza\u00e7\u00e3o com exemplos pr\u00e1ticos: ./k-means/main.md</li> <li>\u00c1rvores de Decis\u00e3o \u2014 projeto e an\u00e1lise: ./Machine-Learning/decision_tree_project.md (c\u00f3digo em <code>docs/Machine-Learning/decision-script.py</code> e <code>docs/projeto/decision_tree_project.py</code>)</li> <li>Random Forest \u2014 ensemble de \u00e1rvores e suas aplica\u00e7\u00f5es: ./random-forest/main.md</li> <li>M\u00e9tricas \u2014 fun\u00e7\u00f5es e exemplifica\u00e7\u00f5es de m\u00e9tricas de avalia\u00e7\u00e3o (acur\u00e1cia, precis\u00e3o, recall, F1, etc.): ./metrics/main.md</li> </ul> <p>Al\u00e9m das p\u00e1ginas acima, o reposit\u00f3rio cont\u00e9m scripts em Python localizados nas pastas correspondentes (<code>docs/k-nearest-neighbor/</code>, <code>docs/k-means/</code>, <code>docs/Machine-Learning/</code>, <code>docs/random-forest/</code>, <code>docs/metrics/</code>). H\u00e1 tamb\u00e9m um conjunto de depend\u00eancias em <code>requirements.txt</code> para facilitar a reprodu\u00e7\u00e3o do ambiente.</p>"},{"location":"#dados","title":"Dados","text":"<p>Os datasets utilizados foram obtidos atravez da plataforma kaggle</p> <ul> <li>Titanic Dataset</li> <li>Ford and mercedes Used Car</li> </ul>"},{"location":"#como-navegar-nesta-documentacao","title":"Como navegar nesta documenta\u00e7\u00e3o","text":"<p>Use a barra lateral do MkDocs (ou o menu superior) para abrir cada t\u00f3pico. Cada se\u00e7\u00e3o cont\u00e9m uma explica\u00e7\u00e3o te\u00f3rica, exemplos de c\u00f3digo e, quando aplic\u00e1vel, scripts execut\u00e1veis que reproduzem os experimentos.</p>"},{"location":"#ferramentas-utilizadas","title":"Ferramentas utilizadas","text":""},{"location":"Machine-Learning/decision_tree_project/","title":"Machine Learning com \u00c1rvore de Decis\u00e3o","text":""},{"location":"Machine-Learning/decision_tree_project/#machine-learning-com-arvore-de-decisao","title":"Machine Learning com \u00c1rvore de Decis\u00e3o","text":""},{"location":"Machine-Learning/decision_tree_project/#dados-utilizados","title":"Dados Utilizados","text":"<p>Dataset - Kaggle</p> <p>Utilizaremos a base de dados de carros usados da BMW para prever a categoria de consumo de combust\u00edvel (baixo, m\u00e9dio ou alto) com base nas caracter\u00edsticas do ve\u00edculo.</p> modelyearpricetransmissionmileagefuelTypetaxmpgengineSize <p>Tipo: categ\u00f3rica nominal O que \u00e9: modelo do carro (1 Series, 3 Series, 5 Series etc.). Para que serve: vari\u00e1vel altamente relevante para pre\u00e7o. A\u00e7\u00e3o necess\u00e1ria: transformar em dummies; verificar categorias muito raras.</p> 2025-12-06T01:51:32.045110 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica discreta O que \u00e9: ano de fabrica\u00e7\u00e3o. Para que serve: influencia fortemente o pre\u00e7o devido \u00e0 deprecia\u00e7\u00e3o. A\u00e7\u00e3o necess\u00e1ria: manter; poss\u00edvel criar \u201cidade = ano_atual \u2013 year\u201d.</p> 2025-12-06T01:51:32.189610 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica cont\u00ednua (target t\u00edpica) O que \u00e9: pre\u00e7o do carro em d\u00f3lares/libra (dependendo da base). Para que serve: vari\u00e1vel dependente caso o modelo seja regress\u00e3o. A\u00e7\u00e3o necess\u00e1ria: checar outliers e distribui\u00e7\u00e3o; poss\u00edvel usar log-transform.</p> 2025-12-06T01:51:32.360366 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica nominal O que \u00e9: tipo de transmiss\u00e3o (Manual, Automatic, Semi-Auto). Para que serve: algumas transmiss\u00f5es valorizam/desvalorizam o ve\u00edculo. A\u00e7\u00e3o necess\u00e1ria: transformar em dummies; checar categorias pouco frequentes.</p> 2025-12-06T01:51:32.484388 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica cont\u00ednua O que \u00e9: quilometragem rodada. Para que serve: altamente correlacionado ao pre\u00e7o (quanto maior, menor o valor). A\u00e7\u00e3o necess\u00e1ria: checar outliers; normaliza\u00e7\u00e3o opcional.</p> 2025-12-06T01:51:32.600734 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica nominal O que \u00e9: tipo de combust\u00edvel (Diesel, Petrol, Hybrid etc.). Para que serve: tem impacto direto no consumo, custo de manuten\u00e7\u00e3o e pre\u00e7o. A\u00e7\u00e3o necess\u00e1ria: dummies; verificar equil\u00edbrio entre categorias.</p> 2025-12-06T01:51:32.726065 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica cont\u00ednua O que \u00e9: imposto anual do ve\u00edculo. Para que serve: relacionado ao consumo/emiss\u00e3o; pode influenciar o valor. A\u00e7\u00e3o necess\u00e1ria: manter; tratar outliers.</p> 2025-12-06T01:51:32.856752 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica cont\u00ednua O que \u00e9: consumo em milhas por gal\u00e3o. Para que serve: efici\u00eancia energ\u00e9tica; compradores valorizam modelos econ\u00f4micos. A\u00e7\u00e3o necess\u00e1ria: manter; verificar valores absurdos.</p> 2025-12-06T01:51:33.066765 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica cont\u00ednua O que \u00e9: tamanho do motor em litros. Para que serve: impacta desempenho e consumo; influencia pre\u00e7o. A\u00e7\u00e3o necess\u00e1ria: manter; checar valores inconsistentes.</p> 2025-12-06T01:51:33.233845 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/"},{"location":"Machine-Learning/decision_tree_project/#modelo-de-arvore-de-decisao-decision-tree","title":"Modelo de \u00c1rvore de Decis\u00e3o (Decision Tree)","text":"<p>Este documento apresenta a implementa\u00e7\u00e3o, explica\u00e7\u00e3o te\u00f3rica e an\u00e1lise do modelo Decision Tree Classifier, aplicado ao dataset BMW Used Cars do Kaggle.  </p>"},{"location":"Machine-Learning/decision_tree_project/#o-que-e-uma-arvore-de-decisao","title":"O que \u00e9 uma \u00c1rvore de Decis\u00e3o?","text":"<p>Uma \u00c1rvore de Decis\u00e3o (Decision Tree) \u00e9 um algoritmo de Machine Learning supervisionado que pode ser usado para:</p> <ul> <li>Classifica\u00e7\u00e3o (prever categorias)</li> <li>Regress\u00e3o (prever valores cont\u00ednuos)</li> </ul> <p>Ela se comporta como uma sequ\u00eancia de perguntas do tipo \"se... ent\u00e3o...\". Cada pergunta divide os dados em grupos mais homog\u00eaneos. Uma \u00e1rvore \u00e9 composta por:</p> <ul> <li>Raiz (root) \u2192 primeira divis\u00e3o  </li> <li>N\u00f3s internos \u2192 perguntas  </li> <li>Folhas (leaves) \u2192 resultado final (classe prevista)</li> </ul>"},{"location":"Machine-Learning/decision_tree_project/#objetivo-do-algoritmo","title":"Objetivo do algoritmo","text":"<p>Criar divis\u00f5es que deixem os grupos o mais puros poss\u00edvel, isto \u00e9, com elementos de uma \u00fanica classe.</p>"},{"location":"Machine-Learning/decision_tree_project/#metricas-usadas-para-decidir-as-divisoes","title":"M\u00e9tricas usadas para decidir as divis\u00f5es","text":"<ul> <li>Gini Impurity </li> <li>Entropia Ambas medem o quanto um conjunto \u00e9 \"misturado\" entre classes. Quanto menor esse valor, mais pura \u00e9 a divis\u00e3o.</li> </ul> <p>\u00c1rvores t\u00eam benef\u00edcios importantes:</p> <ul> <li>Interpretabilidade (gr\u00e1fico f\u00e1cil de entender)</li> <li>N\u00e3o exigem normaliza\u00e7\u00e3o dos dados</li> <li>Aceitam dados num\u00e9ricos e categ\u00f3ricos (com encoding)</li> <li>Capturam rela\u00e7\u00f5es n\u00e3o-lineares</li> </ul>"},{"location":"Machine-Learning/decision_tree_project/#execucao-do-script","title":"Execu\u00e7\u00e3o do Script","text":"<p>A seguir est\u00e1 o c\u00f3digo executado:</p> output <p>Accuracy: 0.97  2025-12-06T01:51:35.749619 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ </p> code <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n\nimport kagglehub\nfrom io import StringIO\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\nplt.figure(figsize=(12, 10))\n\n\npath = kagglehub.dataset_download(\"adityadesai13/used-car-dataset-ford-and-mercedes\")\n\ndf = pd.read_csv(path + \"/bmw.csv\")  \nx = df[['model', 'year', 'price', 'transmission', 'mileage', 'fuelType', 'tax', 'engineSize']]\n\nlabel_encoder = LabelEncoder()\n\n\n\n# Carregar o conjunto de dados\nlabel_encoder = LabelEncoder()\nx['model'] = label_encoder.fit_transform(x['model'])\nx['transmission'] = label_encoder.fit_transform(x['transmission'])\nx['fuelType'] = label_encoder.fit_transform(x['fuelType'])  \n\n#setar a saida\ny= df['consumo_cat'] = pd.cut(\n        df['mpg'],\n        bins=[0, 25, 40, 100],   # faixas (ajust\u00e1veis)\n        labels=['baixo', 'medio', 'alto']\n)\n\n\n\n\ndata = x.copy()\ndata['target'] = y\n\n# Deletar linhas com valores ausentes\ndata = data.dropna()\n\n# Dividir em caracter\u00edsticas (X) e alvo (y)\nx_clean = data.drop('target', axis=1)\ny_clean = data['target']\n\n# Treinar e testar o modelo\nx_train, x_test, y_train, y_test = train_test_split(\n    x_clean, y_clean, \n    test_size=0.7, \n    random_state=42\n)\n\n\n# Criar e treinar o modelo de \u00e1rvore de decis\u00e3o\nclassifier = tree.DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\n\n# Avaliar o modelo\naccuracy = classifier.score(x_test, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\")\ntree.plot_tree(classifier)\n\n# Para imprimir na p\u00e1gina HTML\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\")\nprint(buffer.getvalue())\n</code></pre>"},{"location":"Machine-Learning/decision_tree_project/#passo-a-passo-da-implementacao","title":"Passo a Passo da Implementa\u00e7\u00e3o","text":""},{"location":"Machine-Learning/decision_tree_project/#importar-bibliotecas","title":"Importar bibliotecas","text":"<pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nfrom io import StringIO\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport kagglehub\n\nplt.figure(figsize=(12, 10))\n</code></pre> <p>Explica\u00e7\u00e3o Te\u00f3rica: - <code>DecisionTreeClassifier</code> cria o modelo. - \u00c1rvores n\u00e3o exigem normaliza\u00e7\u00e3o, pois dividem os dados com base em pontos de corte. - <code>LabelEncoder</code> transforma textos em n\u00fameros (necess\u00e1rio porque \u00e1rvores n\u00e3o trabalham com strings). - <code>train_test_split</code> separa os dados de forma aleat\u00f3ria.  </p>"},{"location":"Machine-Learning/decision_tree_project/#carregar-a-base","title":"Carregar a base","text":"<pre><code>path = kagglehub.dataset_download(\"adityadesai13/used-car-dataset-ford-and-mercedes\")\n\ndf = pd.read_csv(path + \"/bmw.csv\")  \nx = df[['model', 'year', 'price', 'transmission', 'mileage', 'fuelType', 'tax', 'engineSize']]\n</code></pre> <p>Explica\u00e7\u00e3o: Carregamos o dataset BMW. Selecionamos atributos que influenciam o consumo do ve\u00edculo.</p>"},{"location":"Machine-Learning/decision_tree_project/#transformar-variaveis-e-definir-o-alvo","title":"Transformar vari\u00e1veis e definir o alvo","text":"<pre><code>label_encoder = LabelEncoder()\nx['model'] = label_encoder.fit_transform(x['model'])\nx['transmission'] = label_encoder.fit_transform(x['transmission'])\nx['fuelType'] = label_encoder.fit_transform(x['fuelType'])  \n\ny = df['consumo_cat'] = pd.cut(\n        df['mpg'],\n        bins=[0, 25, 40, 100],  \n        labels=['baixo', 'medio', 'alto']\n)\n\ndata = x.copy()\ndata['target'] = y\n</code></pre>"},{"location":"Machine-Learning/decision_tree_project/#teoria-aplicada","title":"Teoria aplicada","text":"<p>\u00c1rvores:</p> <ul> <li>lidam bem com vari\u00e1veis categ\u00f3ricas desde que convertidas para n\u00fameros</li> <li>n\u00e3o precisam que os dados sejam escalonados</li> <li>conseguem encontrar intera\u00e7\u00f5es automaticamente, como: <p>alta cilindrada + c\u00e2mbio autom\u00e1tico \u2192 consumo baixo</p> </li> </ul> <p>A vari\u00e1vel alvo \u00e9 categorizada a partir da economia de combust\u00edvel (<code>mpg</code>).</p>"},{"location":"Machine-Learning/decision_tree_project/#remover-valores-ausentes","title":"Remover valores ausentes","text":"<pre><code>data = data.dropna()\n\nx_clean = data.drop('target', axis=1)\ny_clean = data['target']\n</code></pre> <p>Por qu\u00ea? \u00c1rvores n\u00e3o lidam com valores faltantes nativamente. Removemos entradas com <code>NaN</code>.</p>"},{"location":"Machine-Learning/decision_tree_project/#treinar-e-avaliar-o-modelo","title":"Treinar e avaliar o modelo","text":"<pre><code>x_train, x_test, y_train, y_test = train_test_split(\n    x_clean, y_clean, \n    test_size=0.7, \n    random_state=42\n)\n\nclassifier = tree.DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\n\naccuracy = classifier.score(x_test, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\ntree.plot_tree(classifier)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\")\nprint(buffer.getvalue())\n</code></pre>"},{"location":"Machine-Learning/decision_tree_project/#teoria-da-arvore-aplicada-ao-treino","title":"Teoria da \u00c1rvore aplicada ao treino","text":"<p>\u00c1rvore de decis\u00e3o:</p> <ol> <li>Seleciona uma vari\u00e1vel</li> <li>Testa poss\u00edveis cortes</li> <li>Calcula o ganho de impureza</li> <li>Escolhe o melhor ponto</li> <li>Repete o processo at\u00e9:</li> <li>atingir profundidade m\u00e1xima</li> <li>ou n\u00e3o haver melhora</li> </ol> <p>Como n\u00e3o definimos par\u00e2metros, o modelo:</p> <ul> <li>cresce at\u00e9 o limite (tend\u00eancia a overfitting)</li> <li>usa Gini Impurity por padr\u00e3o</li> </ul>"},{"location":"Machine-Learning/decision_tree_project/#interpretacao-dos-resultados","title":"Interpreta\u00e7\u00e3o dos Resultados","text":"<p>A acur\u00e1cia indica o qu\u00e3o bem o modelo classificou as categorias de consumo (<code>baixo</code>, <code>m\u00e9dio</code>, <code>alto</code>).</p>"},{"location":"Machine-Learning/decision_tree_project/#ponto-forte-das-arvores","title":"Ponto forte das \u00e1rvores","text":"<p>Elas conseguem aprender regras como:</p> <ul> <li>Pre\u00e7o alto + motor grande \u2192 consumo baixo</li> <li>Carro novo + motor pequeno \u2192 consumo alto</li> <li>Alta quilometragem \u2192 pior consumo </li> </ul> <p>Essas regras s\u00e3o automaticamente encontradas pelo algoritmo.</p>"},{"location":"Machine-Learning/decision_tree_project/#fragilidade","title":"Fragilidade","text":"<p>Sem limitar: - profundidade - n\u00famero m\u00ednimo de amostras por n\u00f3 a \u00e1rvore pode decorar o treino \u2192 overfitting.</p>"},{"location":"Machine-Learning/decision_tree_project/#conclusao","title":"Conclus\u00e3o","text":"<p>Este projeto mostra:</p> <ul> <li>Como aplicar uma \u00c1rvore de Decis\u00e3o na pr\u00e1tica  </li> <li>Como transformar vari\u00e1veis categ\u00f3ricas  </li> <li>Como criar uma vari\u00e1vel alvo categorizada  </li> <li>Como visualizar e interpretar o modelo  </li> <li>Como avaliar desempenho  </li> </ul> <p>\u00c1rvores s\u00e3o um dos algoritmos mais intuitivos e importantes do Machine Learning moderno, base de modelos mais poderosos como Random Forest e Gradient Boosting.</p>"},{"location":"k-means/main/","title":"K-Means","text":""},{"location":"k-means/main/#documentacao-k-means-clustering-no-titanic","title":"Documenta\u00e7\u00e3o - K-Means Clustering no Titanic","text":"<p>Este script aplica o algoritmo K-Means Clustering ao dataset Titanic, baixado automaticamente do Kaggle via <code>kagglehub</code>. O objetivo \u00e9 agrupar os passageiros com base em caracter\u00edsticas selecionadas e visualizar os clusters.</p>"},{"location":"k-means/main/#dados-utilizados","title":"Dados utilizados","text":"<p>Dataset - Kaggle</p> PassengerIdSurvivedPclassSexAgeSibSpParchTicketFareEmbarked <p>Tipo: num\u00e9rica discreta O que \u00e9: identificador \u00fanico do passageiro. Para que serve: n\u00e3o cont\u00e9m informa\u00e7\u00e3o \u00fatil para predi\u00e7\u00e3o. A\u00e7\u00e3o necess\u00e1ria: remover do modelo.</p> 2025-12-06T01:51:37.676935 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica bin\u00e1ria (target) O que \u00e9: 1 = sobreviveu; 0 = n\u00e3o sobreviveu. Para que serve: vari\u00e1vel dependente a ser prevista. A\u00e7\u00e3o necess\u00e1ria: checar balanceamento (a classe 0 \u00e9 ligeiramente maior).</p> 2025-12-06T01:51:37.776470 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica ordinal O que \u00e9: classe socioecon\u00f4mica do ticket (1\u00aa, 2\u00aa, 3\u00aa). Para que serve: proxy de condi\u00e7\u00e3o financeira/social que influencia sobreviv\u00eancia. A\u00e7\u00e3o necess\u00e1ria: manter como categ\u00f3rica ordinal; verificar distribui\u00e7\u00e3o nas classes.</p> 2025-12-06T01:51:37.839223 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica bin\u00e1ria Oque \u00e9: sexo biol\u00f3gico do passageiro (male/female). Para que serve: uma das vari\u00e1veis mais importantes na sobreviv\u00eancia. A\u00e7\u00e3o necess\u00e1ria: codificar para dummy (female \u2192 1, male \u2192 0).</p> 2025-12-06T01:51:37.902504 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica cont\u00ednua O que \u00e9: idade em anos. Para que serve: importante para separar grupos vulner\u00e1veis (crian\u00e7as, adultos). A\u00e7\u00e3o necess\u00e1ria: 177 valores ausentes \u2192 imputar (m\u00e9dia/mediana ou por t\u00edtulo extra\u00eddo do Name).</p> 2025-12-06T01:51:37.974552 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica discreta O que \u00e9: n\u00famero de irm\u00e3os/c\u00f4njuges a bordo. Para que serve: indica tamanho do grupo familiar; pode influenciar sobreviv\u00eancia. A\u00e7\u00e3o necess\u00e1ria: manter; poss\u00edvel normalizar ou agrupar faixas.</p> 2025-12-06T01:51:38.070661 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica discreta O que \u00e9: n\u00famero de pais/filhos a bordo. Para que serve: outro indicador do grupo familiar. A\u00e7\u00e3o necess\u00e1ria: manter; poss\u00edvel criar \u201cFamilySize = SibSp + Parch + 1\u201d.</p> 2025-12-06T01:51:38.161855 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica (texto) O que \u00e9: n\u00famero/c\u00f3digo do ticket. Para que serve: pouco \u00fatil originalmente; pode ajudar se agrupado por prefixos. A\u00e7\u00e3o necess\u00e1ria: normalmente remover.</p> 2025-12-06T01:51:38.248145 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica cont\u00ednua O que \u00e9: tarifa paga pelo ticket. Para que serve: rela\u00e7\u00e3o com classe social; boa vari\u00e1vel preditiva. A\u00e7\u00e3o necess\u00e1ria: checar outliers; poss\u00edvel normaliza\u00e7\u00e3o logar\u00edtmica.</p> 2025-12-06T01:51:38.347087 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica nominal O que \u00e9: porto de embarque (C, Q, S). Para que serve: pode refletir diferen\u00e7as sociais/regionais. A\u00e7\u00e3o necess\u00e1ria: imputar os 2 valores ausentes; criar dummies.</p> 2025-12-06T01:51:38.516217 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/"},{"location":"k-means/main/#introducao-ao-k-means","title":"Introdu\u00e7\u00e3o ao K-Means","text":"<p>O K-Means \u00e9 um algoritmo de agrupamento n\u00e3o supervisionado, cujo objetivo \u00e9 dividir os dados em k grupos (clusters) de forma que os elementos dentro de um mesmo cluster sejam semelhantes entre si e diferentes dos clusters vizinhos.</p> <p>A ideia central \u00e9:</p> <p>Encontrar centros (chamados de centroides) que representem cada grupo, e atribuir cada ponto ao centro mais pr\u00f3ximo.</p> <p>O K-Means \u00e9 amplamente utilizado em: - segmenta\u00e7\u00e3o de clientes - compress\u00e3o de imagens - agrupamento de usu\u00e1rios - descoberta de padr\u00f5es ocultos em datasets n\u00e3o rotulados  </p> <p>Como o dataset Titanic n\u00e3o possui um r\u00f3tulo de cluster verdadeiro, o objetivo aqui \u00e9 descobrir segmentos naturais entre os passageiros.</p>"},{"location":"k-means/main/#como-o-k-means-funciona-intuicao-matematica-minima","title":"Como o K-Means funciona (Intui\u00e7\u00e3o + Matem\u00e1tica m\u00ednima)","text":""},{"location":"k-means/main/#intuicao-do-algoritmo","title":"Intui\u00e7\u00e3o do algoritmo","text":"<p>O algoritmo segue duas etapas repetidas:</p> <ol> <li>Atribui\u00e7\u00e3o: cada ponto \u00e9 colocado no cluster cujo centr\u00f3ide \u00e9 o mais pr\u00f3ximo.  </li> <li>Atualiza\u00e7\u00e3o: recalculam-se os centr\u00f3ides como a m\u00e9dia dos pontos atribu\u00eddos ao cluster.</li> </ol> <p>Isso \u00e9 repetido at\u00e9 que os centr\u00f3ides n\u00e3o mudem mais ou o n\u00famero m\u00e1ximo de itera\u00e7\u00f5es seja alcan\u00e7ado.</p>"},{"location":"k-means/main/#limitacoes-teoricas-do-k-means","title":"Limita\u00e7\u00f5es te\u00f3ricas do K-Means","text":"<ul> <li>Assume clusters esf\u00e9ricos e de tamanhos semelhantes </li> <li>Sens\u00edvel a outliers </li> <li>Sens\u00edvel \u00e0 inicializa\u00e7\u00e3o \u2192 por isso existe o k-means++ </li> <li>Requer defini\u00e7\u00e3o pr\u00e9via de k </li> <li>Funciona melhor com vari\u00e1veis num\u00e9ricas e escaladas</li> </ul>"},{"location":"k-means/main/#bibliotecas-utilizadas-com-justificativa-teorica","title":"Bibliotecas utilizadas (com justificativa te\u00f3rica)","text":"<ul> <li>pandas \u2192 manipula\u00e7\u00e3o dos dados brutos  </li> <li>matplotlib \u2192 visualiza\u00e7\u00e3o e inspe\u00e7\u00e3o dos clusters  </li> <li>KMeans (scikit-learn) \u2192 implementa\u00e7\u00e3o do algoritmo  </li> <li>StandardScaler \u2192 K-Means exige padroniza\u00e7\u00e3o (dist\u00e2ncias euclidianas)  </li> <li>LabelEncoder \u2192 transforma\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas  </li> <li>kagglehub \u2192 download program\u00e1tico do dataset  </li> <li>StringIO \u2192 suporte \u00e0 exporta\u00e7\u00e3o do gr\u00e1fico no formato SVG  </li> </ul>"},{"location":"k-means/main/#codigo-exemplo-e-execucao","title":"C\u00f3digo Exemplo e execu\u00e7\u00e3o","text":"outputcode <p>Acur\u00e1cia do K-Means para prever 'Survived': 0.67 Matriz de confus\u00e3o: [[450  99]  [191 151]]  2025-12-06T01:51:39.562133 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ </p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport kagglehub\nimport os\nfrom io import StringIO\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\n\npath = kagglehub.dataset_download(\"yasserh/titanic-dataset\")\nfile_path = os.path.join(path, \"Titanic-Dataset.csv\")\ndf = pd.read_csv(file_path)\n\n\n\nfeatures = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']].copy()\nlabels_true = df['Survived'].copy()\n\n# Preprocessamento\nfeatures['Sex'] = LabelEncoder().fit_transform(features['Sex'])\nfeatures['Age'].fillna(features['Age'].median(), inplace=True)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(features)\n\nkmeans = KMeans(n_clusters=2, init='k-means++', max_iter=100, random_state=42, n_init=10)\nlabels = kmeans.fit_predict(X_scaled)\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\n\n\n\n# K-Means\nkmeans = KMeans(n_clusters=2, init='k-means++', max_iter=100, random_state=42, n_init=10)\nlabels_pred = kmeans.fit_predict(X_scaled)\n\n# Ajuste de r\u00f3tulo (clusters podem estar invertidos)\nif accuracy_score(labels_true, labels_pred) &lt; accuracy_score(labels_true, 1-labels_pred):\n    labels_pred = 1-labels_pred\n\n# Avalia\u00e7\u00e3o\nacc = accuracy_score(labels_true, labels_pred)\ncm = confusion_matrix(labels_true, labels_pred)\nprint(f\"Acur\u00e1cia do K-Means para prever 'Survived': {acc:.2f}\")\nprint(\"Matriz de confus\u00e3o:\")\nprint(cm)\n\n# Visualiza\u00e7\u00e3o dos clusters\nx_var = 'Age'\ny_var = 'Fare'\nx_idx = features.columns.get_loc(x_var)\ny_idx = features.columns.get_loc(y_var)\n\nplt.figure(figsize=(10, 8))\nplt.scatter(X_scaled[:, x_idx], X_scaled[:, y_idx], c=labels_pred, cmap='viridis', s=50)\nplt.scatter(kmeans.cluster_centers_[:, x_idx], kmeans.cluster_centers_[:, y_idx],\n            c='red', marker='*', s=200, label='Centroids')\nplt.title(\"K-Means Clustering - Titanic Dataset\")\nplt.xlabel(f\"{x_var} (scaled)\")\nplt.ylabel(f\"{y_var} (scaled)\")\nplt.legend()\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</code></pre>"},{"location":"k-means/main/#download-e-carregamento-do-dataset","title":"Download e carregamento do dataset","text":"<pre><code>path = kagglehub.dataset_download(\"yasserh/titanic-dataset\")\nfile_path = os.path.join(path, \"Titanic-Dataset.csv\")\ndf = pd.read_csv(file_path)\n</code></pre>"},{"location":"k-means/main/#preparacao-dos-dados-com-explicacoes-teoricas","title":"Prepara\u00e7\u00e3o dos dados (com explica\u00e7\u00f5es te\u00f3ricas)","text":""},{"location":"k-means/main/#por-que-preparar-os-dados","title":"\u2714 Por que preparar os dados?","text":"<p>O K-Means n\u00e3o trabalha com dados ausentes, nem com vari\u00e1veis categ\u00f3ricas, e \u00e9 sens\u00edvel \u00e0s escalas. Por isso seguimos estes passos:</p>"},{"location":"k-means/main/#selecao-das-variaveis","title":"\u2714 Sele\u00e7\u00e3o das vari\u00e1veis","text":"<p>Foram escolhidas:</p> <ul> <li>Pclass  </li> <li>Sex  </li> <li>Age  </li> <li>SibSp  </li> <li>Parch  </li> <li>Fare  </li> </ul> <p>Essas vari\u00e1veis representam bem o perfil dos passageiros.</p>"},{"location":"k-means/main/#conversao-de-variavel-categorica","title":"\u2714 Convers\u00e3o de vari\u00e1vel categ\u00f3rica","text":"<p><code>Sex</code> \u2192 0 ou 1.</p>"},{"location":"k-means/main/#tratamento-de-valores-ausentes","title":"\u2714 Tratamento de valores ausentes","text":"<p><code>Age</code> \u2192 preenchimento com a mediana.</p>"},{"location":"k-means/main/#normalizacao-item-critico","title":"\u2714 Normaliza\u00e7\u00e3o (item cr\u00edtico!)","text":"<p>O K-Means usa dist\u00e2ncia euclidiana, ent\u00e3o vari\u00e1veis com magnitudes maiores dominam o clustering. Por isso aplicamos <code>StandardScaler()</code>:</p> <pre><code>features = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']].copy()\nfeatures['Sex'] = LabelEncoder().fit_transform(features['Sex'])\nfeatures['Age'].fillna(features['Age'].median(), inplace=True)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(features)\n</code></pre>"},{"location":"k-means/main/#aplicacao-do-algoritmo-k-means-com-teoria-dos-parametros","title":"Aplica\u00e7\u00e3o do algoritmo K-Means (com teoria dos par\u00e2metros)","text":""},{"location":"k-means/main/#parametros-utilizados","title":"Par\u00e2metros utilizados:","text":"<ul> <li>n_clusters=2 </li> <li>init='k-means++' </li> <li>max_iter=100 </li> <li>random_state=42 </li> <li>n_init=10 </li> </ul> <pre><code>kmeans = KMeans(n_clusters=2, init='k-means++', max_iter=100, random_state=42, n_init=10)\nlabels = kmeans.fit_predict(X_scaled)\n</code></pre>"},{"location":"k-means/main/#visualizacao-dos-clusters","title":"Visualiza\u00e7\u00e3o dos clusters","text":"<pre><code>plt.figure(figsize=(10, 8))\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis', s=50)\n\nplt.scatter(kmeans.cluster_centers_[:, 0], \n            kmeans.cluster_centers_[:, 1],\n            c='red', marker='*', s=200, label='Centroids')\n\nplt.title(\"K-Means Clustering - Titanic Dataset\")\nplt.xlabel(\"Feature 1 (scaled)\")\nplt.ylabel(\"Feature 2 (scaled)\")\nplt.legend()\n</code></pre>"},{"location":"k-means/main/#exportacao-do-grafico-para-mkdocs","title":"Exporta\u00e7\u00e3o do gr\u00e1fico para MkDocs","text":"<pre><code>from io import StringIO\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</code></pre>"},{"location":"k-means/main/#interpretacao-dos-clusters","title":"Interpreta\u00e7\u00e3o dos clusters","text":"<p>Os clusters n\u00e3o representam classes \"verdadeiras\", mas padr\u00f5es naturais dos dados. Com frequ\u00eancia, no Titanic, os agrupamentos tendem a separar:</p> <ul> <li>passageiros de classes mais altas  </li> <li>tarifas maiores  </li> <li>fam\u00edlias menores  </li> <li>perfis de idade distintos  </li> </ul>"},{"location":"k-nearest-neighbor/knn_documentacao/","title":"Machine Learning com KNN (K-Nearest Neighbors)","text":""},{"location":"k-nearest-neighbor/knn_documentacao/#machine-learning-com-knn-k-nearest-neighbors","title":"Machine Learning com KNN (K-Nearest Neighbors)","text":""},{"location":"k-nearest-neighbor/knn_documentacao/#base-de-dados-utilizada","title":"Base de Dados Utilizada","text":"<p>A base de dados utilizada \u00e9 o dataset do Titanic, obtido atrav\u00e9s da biblioteca <code>kagglehub</code>:\\ https://www.kaggle.com/datasets/brendan45774/test-file</p>"},{"location":"k-nearest-neighbor/knn_documentacao/#objetivo-do-projeto","title":"Objetivo do Projeto","text":"<p>O objetivo deste trabalho \u00e9 aplicar o algoritmo K-Nearest Neighbors (KNN) para resolver um problema de Classifica\u00e7\u00e3o Bin\u00e1ria e prever a Sobreviv\u00eancia (Survived) de um passageiro no Titanic.</p> <p>Para simplificar a visualiza\u00e7\u00e3o da fronteira de decis\u00e3o, o modelo utiliza apenas as seguintes features cont\u00ednuas:</p> <ul> <li>Age (Idade do Passageiro)</li> <li>Fare (Pre\u00e7o da Passagem)</li> </ul>"},{"location":"k-nearest-neighbor/knn_documentacao/#analise-e-modelo-knn-k5","title":"An\u00e1lise e Modelo KNN (K=5)","text":"<p>O gr\u00e1fico gerado abaixo representa a fronteira de decis\u00e3o do modelo. As regi\u00f5es coloridas indicam a classe (sobreviv\u00eancia) que o modelo KNN prediz para qualquer novo passageiro que caia naquela \u00e1rea do plano 2D.</p> Resultado Cor de Classifica\u00e7\u00e3o N\u00e3o Sobreviveu Azul claro Sobreviveu Rosa/Vermelho"},{"location":"k-nearest-neighbor/knn_documentacao/#interpretacao-dos-resultados","title":"Interpreta\u00e7\u00e3o dos Resultados","text":"<p>O desempenho do modelo demonstra que a Idade e o Pre\u00e7o da Passagem (Fare) s\u00e3o preditores significativos para a sobreviv\u00eancia no Titanic. Passageiros com passagens mais caras (classes mais altas) tinham maior probabilidade de sobreviver, assim como certos grupos et\u00e1rios.</p> output <p><code>python exec=\"on\" html=\"1\" --8&lt;-- \"./docs/k-nearest-neighbor/knn_script.py\"</code></p> code <p><code>python exec=\"off\" --8&lt;-- \"./docs/k-nearest-neighbor/knn_script.py\"</code></p>"},{"location":"k-nearest-neighbor/knn_documentacao/#passo-a-passo-da-implementacao","title":"Passo a Passo da Implementa\u00e7\u00e3o","text":""},{"location":"k-nearest-neighbor/knn_documentacao/#1-importacao-de-bibliotecas-e-carregamento-de-dados","title":"1. Importa\u00e7\u00e3o de Bibliotecas e Carregamento de Dados","text":"<p>Importamos o StandardScaler, que \u00e9 essencial para o algoritmo KNN.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler # CR\u00cdTICO para KNN\nimport seaborn as sns\nimport pandas as pd\nimport kagglehub\n\n# Carregar o dataset via kagglehub\nkaggle_dataset = kagglehub.dataset_download(\"brendan45774/test-file\")\ndf = pd.read_csv(f\"{kaggle_dataset}/titanic.csv\")\n\n# Selecionar Features (X) e Vari\u00e1vel Alvo (y)\nX = df[['Age', 'Fare']]\ny = df['Survived']  # Vari\u00e1vel bin\u00e1ria: 0 (N\u00e3o Sobreviveu) ou 1 (Sobreviveu)\n\n# Limpeza e Prepara\u00e7\u00e3o\ndata = X.copy()\ndata['Survived'] = y\ndata = data.dropna()  # Remove valores ausentes\n\nX_clean = data[['Age', 'Fare']]\ny_clean = data['Survived']\n</code></pre>"},{"location":"k-nearest-neighbor/knn_documentacao/#2-escalonamento-dos-atributos-standardscaler","title":"2. Escalonamento dos Atributos (StandardScaler)","text":"<p>O KNN \u00e9 baseado na dist\u00e2ncia. Sem esta etapa, a vari\u00e1vel Fare (valores maiores) dominaria o c\u00e1lculo da dist\u00e2ncia em rela\u00e7\u00e3o \u00e0 Age (valores menores), invalidando o modelo.</p> <pre><code># Aplica a Padroniza\u00e7\u00e3o (StandardScaler)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_clean)\nX_scaled_df = pd.DataFrame(X_scaled, columns=X_clean.columns, index=X_clean.index)\n</code></pre>"},{"location":"k-nearest-neighbor/knn_documentacao/#3-treinamento-e-avaliacao-do-modelo","title":"3. Treinamento e Avalia\u00e7\u00e3o do Modelo","text":"<p>Dividimos os dados escalonados (<code>X_scaled_df</code>) e treinamos o <code>KNeighborsClassifier</code> com K=5 vizinhos.</p> <pre><code># Divis\u00e3o Treino/Teste (70% Treino, 30% Teste)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled_df, y_clean,\n    test_size=0.3,\n    random_state=42\n)\n\n# Treinar o modelo KNN\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\n# Prever e Avaliar\npredictions = knn.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions):.2f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, predictions, target_names=['N\u00e3o Sobreviveu', 'Sobreviveu']))\n</code></pre>"},{"location":"k-nearest-neighbor/knn_documentacao/#4-geracao-da-fronteira-de-decisao","title":"4. Gera\u00e7\u00e3o da Fronteira de Decis\u00e3o","text":"<p>Este passo gera a grade de visualiza\u00e7\u00e3o para plotar a fronteira de decis\u00e3o do modelo KNN.</p> <pre><code>plt.figure(figsize=(12, 10))\n\n# Amostragem para plotagem\nsample_size = 5000\nX_vis = X_scaled_df.sample(sample_size, random_state=42)\ny_vis = y_clean.loc[X_vis.index]\n\n# Defini\u00e7\u00e3o dos limites da grade\nh = 0.05\nx_min, x_max = X_scaled_df.iloc[:, 0].min() - 0.5, X_scaled_df.iloc[:, 0].max() + 0.5\ny_min, y_max = X_scaled_df.iloc[:, 1].min() - 0.5, X_scaled_df.iloc[:, 1].max() + 0.5\n\n# Cria\u00e7\u00e3o da grade e predi\u00e7\u00e3o\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\nZ = knn.predict(grid_points).reshape(xx.shape)\n\n# Plot da fronteira e dos dados\nplt.contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=0.3)\nsns.scatterplot(x=X_vis.iloc[:, 0], y=X_vis.iloc[:, 1], hue=y_vis, \n                style=y_vis, palette={0: 'blue', 1: 'orange'}, \n                s=100, markers={0: 'o', 1: 'x'}, legend='full')\n\nplt.xlabel(\"Age (Scaled)\")\nplt.ylabel(\"Fare (Scaled)\")\nplt.title(\"KNN Decision Boundary (k=5) - Titanic Dataset\")\nplt.legend(title=\"Titanic - Survivability\", labels=[\"N\u00e3o Sobreviveu\", \"Sobreviveu\"])\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"k-nearest-neighbor/main/","title":"KNN","text":""},{"location":"k-nearest-neighbor/main/#machine-learning-com-knn-k-nearest-neighbors","title":"Machine Learning com KNN (K-Nearest Neighbors)","text":""},{"location":"k-nearest-neighbor/main/#base-de-dados-utilizada","title":"Base de Dados Utilizada","text":"<p>Dataset - Kaggle</p> PassengerIdSurvivedPclassSexAgeSibSpParchTicketFareEmbarked <p>Tipo: num\u00e9rica discreta O que \u00e9: identificador \u00fanico do passageiro. Para que serve: n\u00e3o cont\u00e9m informa\u00e7\u00e3o \u00fatil para predi\u00e7\u00e3o. A\u00e7\u00e3o necess\u00e1ria: remover do modelo.</p> 2025-12-06T01:51:39.789890 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica bin\u00e1ria (target) O que \u00e9: 1 = sobreviveu; 0 = n\u00e3o sobreviveu. Para que serve: vari\u00e1vel dependente a ser prevista. A\u00e7\u00e3o necess\u00e1ria: checar balanceamento (a classe 0 \u00e9 ligeiramente maior).</p> 2025-12-06T01:51:39.877797 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica ordinal O que \u00e9: classe socioecon\u00f4mica do ticket (1\u00aa, 2\u00aa, 3\u00aa). Para que serve: proxy de condi\u00e7\u00e3o financeira/social que influencia sobreviv\u00eancia. A\u00e7\u00e3o necess\u00e1ria: manter como categ\u00f3rica ordinal; verificar distribui\u00e7\u00e3o nas classes.</p> 2025-12-06T01:51:39.939740 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica bin\u00e1ria Oque \u00e9: sexo biol\u00f3gico do passageiro (male/female). Para que serve: uma das vari\u00e1veis mais importantes na sobreviv\u00eancia. A\u00e7\u00e3o necess\u00e1ria: codificar para dummy (female \u2192 1, male \u2192 0).</p> 2025-12-06T01:51:40.000728 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica cont\u00ednua O que \u00e9: idade em anos. Para que serve: importante para separar grupos vulner\u00e1veis (crian\u00e7as, adultos). A\u00e7\u00e3o necess\u00e1ria: 177 valores ausentes \u2192 imputar (m\u00e9dia/mediana ou por t\u00edtulo extra\u00eddo do Name).</p> 2025-12-06T01:51:40.072552 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica discreta O que \u00e9: n\u00famero de irm\u00e3os/c\u00f4njuges a bordo. Para que serve: indica tamanho do grupo familiar; pode influenciar sobreviv\u00eancia. A\u00e7\u00e3o necess\u00e1ria: manter; poss\u00edvel normalizar ou agrupar faixas.</p> 2025-12-06T01:51:40.168175 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica discreta O que \u00e9: n\u00famero de pais/filhos a bordo. Para que serve: outro indicador do grupo familiar. A\u00e7\u00e3o necess\u00e1ria: manter; poss\u00edvel criar \u201cFamilySize = SibSp + Parch + 1\u201d.</p> 2025-12-06T01:51:40.258926 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica (texto) O que \u00e9: n\u00famero/c\u00f3digo do ticket. Para que serve: pouco \u00fatil originalmente; pode ajudar se agrupado por prefixos. A\u00e7\u00e3o necess\u00e1ria: normalmente remover.</p> 2025-12-06T01:51:40.345342 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica cont\u00ednua O que \u00e9: tarifa paga pelo ticket. Para que serve: rela\u00e7\u00e3o com classe social; boa vari\u00e1vel preditiva. A\u00e7\u00e3o necess\u00e1ria: checar outliers; poss\u00edvel normaliza\u00e7\u00e3o logar\u00edtmica.</p> 2025-12-06T01:51:40.443777 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica nominal O que \u00e9: porto de embarque (C, Q, S). Para que serve: pode refletir diferen\u00e7as sociais/regionais. A\u00e7\u00e3o necess\u00e1ria: imputar os 2 valores ausentes; criar dummies.</p> 2025-12-06T01:51:40.526291 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/"},{"location":"k-nearest-neighbor/main/#objetivo-do-projeto","title":"Objetivo do projeto","text":"<p>Aplicar o algoritmo K-Nearest Neighbors (KNN) para um problema de classifica\u00e7\u00e3o bin\u00e1ria: prever a vari\u00e1vel <code>Survived</code> do dataset Titanic. Para facilitar a visualiza\u00e7\u00e3o da fronteira de decis\u00e3o, o modelo usa apenas duas features cont\u00ednuas: Age e Fare.</p>"},{"location":"k-nearest-neighbor/main/#intuicao-do-knn","title":"Intui\u00e7\u00e3o do KNN","text":"<p>KNN \u00e9 um m\u00e9todo baseado em inst\u00e2ncias (lazy learning). Para classificar um ponto novo, o algoritmo encontra os <code>k</code> pontos mais pr\u00f3ximos no conjunto de treino (vizinhos) e decide a classe pela maioria entre esses vizinhos (classifica\u00e7\u00e3o) ou pela m\u00e9dia (regress\u00e3o). A premissa \u00e9 que pontos pr\u00f3ximos no espa\u00e7o de features tendem a ter o mesmo r\u00f3tulo.</p>"},{"location":"k-nearest-neighbor/main/#por-que-escalonar-standardscaler-e-critico","title":"Por que escalonar (StandardScaler) \u00e9 cr\u00edtico","text":"<p>KNN usa dist\u00e2ncias; se duas features t\u00eam escalas muito diferentes (ex.: <code>Fare</code> em centenas vs <code>Age</code> em dezenas), a feature com maior escala dominar\u00e1 a dist\u00e2ncia e ir\u00e1 enviesar a decis\u00e3o. O <code>StandardScaler</code> transforma cada feature de modo que todas tenham m\u00e9dia 0 e desvio padr\u00e3o 1, tornando as features compar\u00e1veis.</p>"},{"location":"k-nearest-neighbor/main/#complexidade-e-limitacoes-praticas","title":"Complexidade e limita\u00e7\u00f5es pr\u00e1ticas","text":"<ul> <li>Complexidade na predi\u00e7\u00e3o: O(n \u00b7 d) por inst\u00e2ncia (n = n\u00ba de amostras de treino, d = dimens\u00e3o). Para muitos pontos de treino, predi\u00e7\u00e3o fica cara.  </li> <li>Curse of dimensionality: em alta dimens\u00e3o, dist\u00e2ncias tornam-se menos discriminativas \u2192 KNN perde efic\u00e1cia.  </li> <li>Sens\u00edvel a ru\u00eddo e outliers: vizinhos ruidosos mudam a predi\u00e7\u00e3o.  </li> <li>Escolha de k: pequeno \u2192 mais variance (overfitting); grande \u2192 mais bias (underfitting).  </li> <li>Balanceamento de classes: em caso de classes desbalanceadas, vizinhan\u00e7a pode ser dominada pela classe majorit\u00e1ria local.</li> </ul> <p>Acelera\u00e7\u00f5es: KD-Tree, Ball-Tree, aproxima\u00e7\u00f5es (approx. nearest neighbors) ou redu\u00e7\u00e3o de dimensionalidade (PCA, UMAP).</p>"},{"location":"k-nearest-neighbor/main/#hiperparametros-importantes-e-tuning","title":"Hiperpar\u00e2metros importantes e tuning","text":"<ul> <li><code>n_neighbors (k)</code>: testar via valida\u00e7\u00e3o cruzada (CV). Valores comuns: 3,5,7,9.  </li> <li><code>metric</code>: dist\u00e2ncia a usar (euclidiana, manhattan, minkowski).  </li> <li><code>weights</code>: <code>uniform</code> (voto igual) ou <code>distance</code> (vizinhos mais pr\u00f3ximos t\u00eam mais peso).  </li> <li>Pr\u00e9-processamento: escolha de scaler, tratamento de outliers, e engenharia de features.</li> </ul> <p>Dica pr\u00e1tica: use <code>GridSearchCV</code> ou <code>cross_val_score</code> para escolher <code>k</code> com base em m\u00e9tricas (accuracy, F1, ROC-AUC).</p>"},{"location":"k-nearest-neighbor/main/#codigo-exemplo-e-grafico","title":"C\u00f3digo Exemplo e gr\u00e1fico","text":"outputcode <p>Accuracy (KNN com k=5): 0.66  Classification Report:                 precision    recall  f1-score   support  N\u00e3o Sobreviveu       0.68      0.77      0.72       126     Sobreviveu       0.60      0.49      0.54        89        accuracy                           0.66       215      macro avg       0.64      0.63      0.63       215   weighted avg       0.65      0.66      0.65       215   Confusion Matrix: [[97 29]  [45 44]]  2025-12-06T01:51:41.015949 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ </p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport pandas as pd\nimport kagglehub\nimport os\n\n# 1. CARREGAMENTO E PR\u00c9-PROCESSAMENTO DE DADOS\n\n# Baixar o dataset Titanic via kagglehub\npath = kagglehub.dataset_download(\"yasserh/titanic-dataset\")\nfile_path = os.path.join(path, \"Titanic-Dataset.csv\")\ndf = pd.read_csv(file_path)\n\n# Selecionar features num\u00e9ricas: Age, Fare, Pclass\n# Vari\u00e1vel alvo bin\u00e1ria: Survived (0 = n\u00e3o sobreviveu, 1 = sobreviveu)\nX = df[['Pclass', 'Age', 'Fare']].copy()\n\n# Vari\u00e1vel alvo bin\u00e1ria\ny = df['Survived'].copy()\n\n# Limpeza dos dados (remover linhas com valores faltantes)\ndata = X.copy()\ndata['Survived'] = y\ndata = data.dropna()\n\nX_clean = data[['Pclass', 'Age', 'Fare']]\ny_clean = data['Survived']\n\n# 2. ESCALONAMENTO DOS DADOS (CR\u00cdTICO PARA KNN)\nscaler = StandardScaler()\n# Ajustar e transformar as features\nX_scaled = scaler.fit_transform(X_clean)\nX_scaled_df = pd.DataFrame(X_scaled, columns=X_clean.columns, index=X_clean.index)\n\n\n# 3. DIVIS\u00c3O TREINO/TESTE\n# Usamos os dados escalonados (X_scaled_df) para o treino\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled_df, y_clean,\n    test_size=0.3,\n    random_state=42\n)\n\n# 4. TREINAMENTO E AVALIA\u00c7\u00c3O DO MODELO KNN\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\n# M\u00e9tricas de desempenho\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy (KNN com k=5): {accuracy:.2f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, predictions, target_names=['N\u00e3o Sobreviveu', 'Sobreviveu']))\n\n# Matriz de confus\u00e3o\ncm = confusion_matrix(y_test, predictions)\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n\n\n# 5. VISUALIZA\u00c7\u00c3O DA FRONTEIRA DE DECIS\u00c3O\n\n# Para visualiza\u00e7\u00e3o em 2D, vamos usar apenas Age e Fare\nX_2d = X_scaled_df[['Age', 'Fare']].copy()\nX_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(\n    X_2d, y_clean,\n    test_size=0.3,\n    random_state=42\n)\n\n# Treinar KNN com os dados 2D\nknn_2d = KNeighborsClassifier(n_neighbors=5)\nknn_2d.fit(X_train_2d, y_train_2d)\n\n# Reduzir a amostra para visualiza\u00e7\u00e3o (para o scatter plot)\nsample_size = min(1000, len(X_2d))\n# Usamos o \u00edndice para garantir que X_vis e y_vis correspondam\nX_vis = X_2d.sample(sample_size, random_state=42)\ny_vis = y_clean.loc[X_vis.index]\n\nplt.figure(figsize=(12, 8))\n\n# Definir os limites da grade a partir dos dados ESCALONADOS\nh = 0.05 \nx_min, x_max = X_2d['Age'].min() - 0.5, X_2d['Age'].max() + 0.5\ny_min, y_max = X_2d['Fare'].min() - 0.5, X_2d['Fare'].max() + 0.5\n\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Criar a grade de pontos para predi\u00e7\u00e3o\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\n\n# Previs\u00e3o KNN para cada ponto da grade\nZ = knn_2d.predict(grid_points)\nZ = Z.reshape(xx.shape)\n\n# Plot da fronteira e dos pontos\nplt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.3)\nsns.scatterplot(x=X_vis['Age'], y=X_vis['Fare'], hue=y_vis, style=y_vis,\n                palette=\"deep\", s=100, legend='full')\n\n# Configurar legenda e r\u00f3tulos\nhandles, _ = plt.gca().get_legend_handles_labels()\nplt.legend(handles=handles, labels=['N\u00e3o Sobreviveu', 'Sobreviveu'], title=\"Titanic - Survivability\")\n\nplt.xlabel(\"Age (Scaled)\")\nplt.ylabel(\"Fare (Scaled)\")\nplt.title(\"KNN Decision Boundary (k=5) - Titanic Dataset\")\n\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</code></pre>"},{"location":"k-nearest-neighbor/main/#interpretacao-dos-resultados","title":"Interpreta\u00e7\u00e3o dos resultados","text":"<ul> <li>Accuracy e classification report d\u00e3o uma vis\u00e3o global do desempenho (precision, recall, F1) por classe.  </li> <li>Fronteira de decis\u00e3o: ilustra como o modelo divide o espa\u00e7o <code>Age \u00d7 Fare</code>.  </li> <li>Insights pr\u00e1ticos: passageiro com <code>Fare</code> escalado alto tende a ser classificado como <code>Sobreviveu</code> (reflexo de classes sociais/posicionamento no navio). Padr\u00f5es por idade tamb\u00e9m aparecem (ex.: grupos jovens ou muito idosos podem ter maior ou menor probabilidade, dependendo dos vizinhos).</li> </ul> <p>Lembre-se: com apenas duas features, o modelo captura apenas parte da realidade \u2014 usar mais features melhora (ou complica) o que o KNN enxerga.</p>"},{"location":"k-nearest-neighbor/main/#boas-praticas","title":"Boas pr\u00e1ticas","text":"<ul> <li>Escolha de k: avaliar via valida\u00e7\u00e3o cruzada (ex.: testar k em [1,3,5,7,9,11]).  </li> <li>Weights: testar <code>weights='distance'</code> para dar mais peso a vizinhos mais pr\u00f3ximos.  </li> <li>M\u00e9tricas: al\u00e9m de accuracy, usar ROC-AUC (quando adequado), F1 (em classes desbalanceadas).  </li> <li>Balanceamento: em datasets com classes desbalanceadas, considerar undersampling/oversampling ou m\u00e9tricas balanceadas.  </li> <li>Redu\u00e7\u00e3o de dimensionalidade: se aumentar features, usar PCA/TSNE para visualiza\u00e7\u00e3o e possivelmente acelerar.  </li> <li>Valida\u00e7\u00e3o por bootstrap ou k-fold: para estimar variabilidade da m\u00e9trica.  </li> <li>Acelera\u00e7\u00e3o: kd_tree/ball_tree ou Approx Nearest Neighbors para muitos exemplos.</li> </ul>"},{"location":"k-nearest-neighbor/main/#conclusao","title":"Conclus\u00e3o","text":"<p>KNN \u00e9 simples, interpret\u00e1vel e poderoso em espa\u00e7os de baixa dimens\u00e3o com muitos exemplos. No entanto, n\u00e3o escala bem para grandes bases e perde desempenho em alta dimens\u00e3o. No contexto do Titanic com <code>Age</code> e <code>Fare</code>, KNN oferece uma boa forma de visualizar como idade e tarifa se relacionam com a sobreviv\u00eancia, mas deve ser complementado com experimentos (mais features, tuning de k e valida\u00e7\u00e3o) para tirar conclus\u00f5es robustas.</p>"},{"location":"metrics/main/","title":"Metrics","text":""},{"location":"metrics/main/#k-means","title":"K-Means","text":"outputcode <p>Acur\u00e1cia do K-Means para prever 'Survived': 0.67 Matriz de confus\u00e3o: [[450  99]  [191 151]]  2025-12-06T01:51:41.764371 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ </p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport kagglehub\nimport os\nfrom io import StringIO\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\n\npath = kagglehub.dataset_download(\"yasserh/titanic-dataset\")\nfile_path = os.path.join(path, \"Titanic-Dataset.csv\")\ndf = pd.read_csv(file_path)\n\n\n\nfeatures = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']].copy()\nlabels_true = df['Survived'].copy()\n\n# Preprocessamento\nfeatures['Sex'] = LabelEncoder().fit_transform(features['Sex'])\nfeatures['Age'].fillna(features['Age'].median(), inplace=True)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(features)\n\nkmeans = KMeans(n_clusters=2, init='k-means++', max_iter=100, random_state=42, n_init=10)\nlabels = kmeans.fit_predict(X_scaled)\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\n\n\n\n# K-Means\nkmeans = KMeans(n_clusters=2, init='k-means++', max_iter=100, random_state=42, n_init=10)\nlabels_pred = kmeans.fit_predict(X_scaled)\n\n# Ajuste de r\u00f3tulo (clusters podem estar invertidos)\nif accuracy_score(labels_true, labels_pred) &lt; accuracy_score(labels_true, 1-labels_pred):\n    labels_pred = 1-labels_pred\n\n# Avalia\u00e7\u00e3o\nacc = accuracy_score(labels_true, labels_pred)\ncm = confusion_matrix(labels_true, labels_pred)\nprint(f\"Acur\u00e1cia do K-Means para prever 'Survived': {acc:.2f}\")\nprint(\"Matriz de confus\u00e3o:\")\nprint(cm)\n\n# Visualiza\u00e7\u00e3o dos clusters\nx_var = 'Age'\ny_var = 'Fare'\nx_idx = features.columns.get_loc(x_var)\ny_idx = features.columns.get_loc(y_var)\n\nplt.figure(figsize=(10, 8))\nplt.scatter(X_scaled[:, x_idx], X_scaled[:, y_idx], c=labels_pred, cmap='viridis', s=50)\nplt.scatter(kmeans.cluster_centers_[:, x_idx], kmeans.cluster_centers_[:, y_idx],\n            c='red', marker='*', s=200, label='Centroids')\nplt.title(\"K-Means Clustering - Titanic Dataset\")\nplt.xlabel(f\"{x_var} (scaled)\")\nplt.ylabel(f\"{y_var} (scaled)\")\nplt.legend()\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</code></pre>"},{"location":"metrics/main/#knn","title":"KNN","text":"outputcode <p>Accuracy (KNN com k=5): 0.66  Classification Report:                 precision    recall  f1-score   support  N\u00e3o Sobreviveu       0.68      0.77      0.72       126     Sobreviveu       0.60      0.49      0.54        89        accuracy                           0.66       215      macro avg       0.64      0.63      0.63       215   weighted avg       0.65      0.66      0.65       215   Confusion Matrix: [[97 29]  [45 44]]  2025-12-06T01:51:42.187907 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ </p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport pandas as pd\nimport kagglehub\nimport os\n\n# 1. CARREGAMENTO E PR\u00c9-PROCESSAMENTO DE DADOS\n\n# Baixar o dataset Titanic via kagglehub\npath = kagglehub.dataset_download(\"yasserh/titanic-dataset\")\nfile_path = os.path.join(path, \"Titanic-Dataset.csv\")\ndf = pd.read_csv(file_path)\n\n# Selecionar features num\u00e9ricas: Age, Fare, Pclass\n# Vari\u00e1vel alvo bin\u00e1ria: Survived (0 = n\u00e3o sobreviveu, 1 = sobreviveu)\nX = df[['Pclass', 'Age', 'Fare']].copy()\n\n# Vari\u00e1vel alvo bin\u00e1ria\ny = df['Survived'].copy()\n\n# Limpeza dos dados (remover linhas com valores faltantes)\ndata = X.copy()\ndata['Survived'] = y\ndata = data.dropna()\n\nX_clean = data[['Pclass', 'Age', 'Fare']]\ny_clean = data['Survived']\n\n# 2. ESCALONAMENTO DOS DADOS (CR\u00cdTICO PARA KNN)\nscaler = StandardScaler()\n# Ajustar e transformar as features\nX_scaled = scaler.fit_transform(X_clean)\nX_scaled_df = pd.DataFrame(X_scaled, columns=X_clean.columns, index=X_clean.index)\n\n\n# 3. DIVIS\u00c3O TREINO/TESTE\n# Usamos os dados escalonados (X_scaled_df) para o treino\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled_df, y_clean,\n    test_size=0.3,\n    random_state=42\n)\n\n# 4. TREINAMENTO E AVALIA\u00c7\u00c3O DO MODELO KNN\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\n# M\u00e9tricas de desempenho\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy (KNN com k=5): {accuracy:.2f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, predictions, target_names=['N\u00e3o Sobreviveu', 'Sobreviveu']))\n\n# Matriz de confus\u00e3o\ncm = confusion_matrix(y_test, predictions)\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n\n\n# 5. VISUALIZA\u00c7\u00c3O DA FRONTEIRA DE DECIS\u00c3O\n\n# Para visualiza\u00e7\u00e3o em 2D, vamos usar apenas Age e Fare\nX_2d = X_scaled_df[['Age', 'Fare']].copy()\nX_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(\n    X_2d, y_clean,\n    test_size=0.3,\n    random_state=42\n)\n\n# Treinar KNN com os dados 2D\nknn_2d = KNeighborsClassifier(n_neighbors=5)\nknn_2d.fit(X_train_2d, y_train_2d)\n\n# Reduzir a amostra para visualiza\u00e7\u00e3o (para o scatter plot)\nsample_size = min(1000, len(X_2d))\n# Usamos o \u00edndice para garantir que X_vis e y_vis correspondam\nX_vis = X_2d.sample(sample_size, random_state=42)\ny_vis = y_clean.loc[X_vis.index]\n\nplt.figure(figsize=(12, 8))\n\n# Definir os limites da grade a partir dos dados ESCALONADOS\nh = 0.05 \nx_min, x_max = X_2d['Age'].min() - 0.5, X_2d['Age'].max() + 0.5\ny_min, y_max = X_2d['Fare'].min() - 0.5, X_2d['Fare'].max() + 0.5\n\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Criar a grade de pontos para predi\u00e7\u00e3o\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\n\n# Previs\u00e3o KNN para cada ponto da grade\nZ = knn_2d.predict(grid_points)\nZ = Z.reshape(xx.shape)\n\n# Plot da fronteira e dos pontos\nplt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.3)\nsns.scatterplot(x=X_vis['Age'], y=X_vis['Fare'], hue=y_vis, style=y_vis,\n                palette=\"deep\", s=100, legend='full')\n\n# Configurar legenda e r\u00f3tulos\nhandles, _ = plt.gca().get_legend_handles_labels()\nplt.legend(handles=handles, labels=['N\u00e3o Sobreviveu', 'Sobreviveu'], title=\"Titanic - Survivability\")\n\nplt.xlabel(\"Age (Scaled)\")\nplt.ylabel(\"Fare (Scaled)\")\nplt.title(\"KNN Decision Boundary (k=5) - Titanic Dataset\")\n\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</code></pre>"},{"location":"metrics/main/#avaliacao","title":"Avalia\u00e7\u00e3o","text":""},{"location":"metrics/main/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O objetivo desta atividade \u00e9 utilizar dois algoritmos de Machine Learning --- KNN (K-Nearest Neighbors) e K-Means Clustering --- para prever uma vari\u00e1vel categ\u00f3rica.\\ O dataset escolhido foi o Titanic, cujo objetivo principal \u00e9 prever a vari\u00e1vel Survived, que indica se um passageiro sobreviveu (1) ou n\u00e3o (0).</p>"},{"location":"metrics/main/#1-dataset-utilizado","title":"1. Dataset Utilizado","text":"<p>O dataset do Titanic cont\u00e9m informa\u00e7\u00f5es como idade, tarifa paga, classe, sexo, entre outros atributos.\\ Para este projeto, utilizamos especialmente as vari\u00e1veis Age e Fare, devidamente normalizadas, para visualizar o comportamento dos algoritmos.</p>"},{"location":"metrics/main/#2-modelo-k-means","title":"2. Modelo K-Means","text":""},{"location":"metrics/main/#21-objetivo","title":"2.1 Objetivo","text":"<p>Embora o K-Means seja um algoritmo n\u00e3o supervisionado, ele pode ser usado para tentar identificar agrupamentos que se aproximem da vari\u00e1vel categ\u00f3rica \"Survived\".</p>"},{"location":"metrics/main/#22-resultados","title":"2.2 Resultados","text":"<ul> <li> <p>Acur\u00e1cia: 0.67\\</p> </li> <li> <p>Matriz de Confus\u00e3o:</p> <pre><code>[[450  99]\n [191 151]]\n</code></pre> </li> </ul>"},{"location":"metrics/main/#23-interpretacao","title":"2.3 Interpreta\u00e7\u00e3o","text":"<ul> <li> <p>O K-Means conseguiu formar dois clusters que, parcialmente, se     alinham \u00e0s classes de sobreviv\u00eancia.</p> </li> <li> <p>O gr\u00e1fico demonstra a distribui\u00e7\u00e3o dos passageiros por \"Age\" e     \"Fare\", com os centr\u00f3ides marcados em vermelho.</p> </li> </ul>"},{"location":"metrics/main/#3-modelo-knn-k-5","title":"3. Modelo KNN (k = 5)","text":""},{"location":"metrics/main/#31-objetivo","title":"3.1 Objetivo","text":"<p>O KNN \u00e9 um algoritmo supervisionado que classifica um novo ponto com base na proximidade de seus vizinhos mais pr\u00f3ximos.\\ Neste caso, buscamos prever diretamente a vari\u00e1vel Survived.</p>"},{"location":"metrics/main/#32-resultados","title":"3.2 Resultados","text":"<ul> <li> <p>Acur\u00e1cia: 0.65\\</p> </li> <li> <p>Classification Report:</p> <pre><code>N\u00e3o Sobreviveu: precision=0.68, recall=0.75, f1=0.72\nSobreviveu:    precision=0.59, recall=0.51, f1=0.55\n</code></pre> </li> <li> <p>Matriz de Confus\u00e3o:</p> <pre><code>[[95 31]\n [44 45]]\n</code></pre> </li> </ul>"},{"location":"metrics/main/#33-interpretacao","title":"3.3 Interpreta\u00e7\u00e3o","text":"<ul> <li>O modelo tem desempenho moderado, com melhor recall para a classe     \"N\u00e3o Sobreviveu\".</li> <li>O gr\u00e1fico mostra a fronteira de decis\u00e3o do KNN, onde \u00e1reas s\u00e3o     classificadas como propensas \u00e0 sobreviv\u00eancia ou n\u00e3o.</li> <li>A distribui\u00e7\u00e3o dos passageiros demonstra forte sobreposi\u00e7\u00e3o entre     classes, dificultando o trabalho do KNN.</li> </ul>"},{"location":"metrics/main/#4-comparacao-geral-entre-k-means-e-knn","title":"4. Compara\u00e7\u00e3o Geral entre K-Means e KNN","text":"<p>Crit\u00e9rio          K-Means              KNN</p> <p>Tipo              N\u00e3o supervisionado   Supervisionado   Objetivo          Criar clusters       Classificar diretamente   Acur\u00e1cia obtida   0.67                 0.65   Utilidade         Explorar padr\u00f5es     Predi\u00e7\u00e3o efetiva</p>"},{"location":"metrics/main/#observacoes","title":"Observa\u00e7\u00f5es","text":"<ul> <li>Mesmo sendo n\u00e3o supervisionado, o K-Means obteve acur\u00e1cia semelhante     ao KNN, o que demonstra forte sobreposi\u00e7\u00e3o de padr\u00f5es no dataset.</li> <li>O KNN sofre com limites pouco definidos entre as classes, como visto     na fronteira de decis\u00e3o.</li> </ul>"},{"location":"metrics/main/#5-conclusao","title":"5. Conclus\u00e3o","text":"<p>Ambos os modelos apresentaram desempenho similar, mas o KNN \u00e9 mais apropriado para a tarefa, pois \u00e9 supervisionado.\\ O K-Means, apesar de n\u00e3o ser ideal para classifica\u00e7\u00e3o, oferece insights sobre agrupamentos naturais no dataset.</p> <p>O experimento ilustra bem as diferen\u00e7as entre algoritmos supervisionados e n\u00e3o supervisionados, bem como suas limita\u00e7\u00f5es ao lidar com dados reais e complexos como o Titanic.</p>"},{"location":"pagerank/main/","title":"Page-Rank","text":""},{"location":"pagerank/main/#page-rank","title":"Page Rank","text":"<p>O PageRank \u00e9 um algoritmo que mede a import\u00e2ncia relativa de cada n\u00f3 dentro de um grafo. A import\u00e2ncia de um n\u00f3 \u00e9 determinada pela quantidade e qualidade dos links que apontam para ele.</p> <p>Os resultados apresentados na imagem e gerados c\u00f3digo a seguir mostram os valores finais de PageRank para os n\u00f3s A, B, C e D.</p> <pre><code>graph = {\n    \"A\": [\"B\", \"C\"],\n    \"B\": [\"C\"],\n    \"C\": [\"A\", \"D\"],\n    \"D\": [\"C\"]\n}\n</code></pre>"},{"location":"pagerank/main/#codigo-do-page-rank","title":"Codigo do Page Rank","text":"outputcode <p>PageRank Convergido: A 0.2199 B 0.131 C 0.4292 D 0.2199  Compara\u00e7\u00e3o com NetworkX: PageRank NetworkX: A 0.2199 B 0.131 C 0.4292 D 0.2199 </p> <pre><code>import numpy as np\n\n# Grafo representado como dicion\u00e1rio: cada n\u00f3 tem sua lista de links de sa\u00edda\ngraph = {\n    \"A\": [\"B\", \"C\"],\n    \"B\": [\"C\"],\n    \"C\": [\"A\", \"D\"],\n    \"D\": [\"C\"]\n}\n\nnodes = list(graph.keys())\nn = len(nodes)\n\n# Par\u00e2metros\nd = 0.85   # damping factor\nepsilon = 1e-6  # crit\u00e9rio de converg\u00eancia\n\n# Inicializa\u00e7\u00e3o: todos come\u00e7am com 1/n\npagerank = {node: 1/n for node in nodes}\n\nconverged = False\n\nwhile not converged:\n    new_pr = {}\n\n    for node in nodes:\n        # Teletransporte\n        pr_value = (1 - d) / n\n\n        # Somat\u00f3rio das contribui\u00e7\u00f5es\n        for other in nodes:\n            if node in graph[other]:  # se other aponta para node\n                pr_value += d * (pagerank[other] / len(graph[other]))\n\n        new_pr[node] = pr_value\n\n    # Verificar converg\u00eancia\n    diff = sum(abs(new_pr[node] - pagerank[node]) for node in nodes)\n\n    pagerank = new_pr\n\n    if diff &lt; epsilon:\n        converged = True\n\nprint(\"PageRank Convergido:\")\nfor node, pr in pagerank.items():\n    print(node, round(pr, 4))\n\n\n\n\nprint(\"\\nCompara\u00e7\u00e3o com NetworkX:\")\n\n\n\n\nimport networkx as nx\n\nG = nx.DiGraph()\n\nfor node, links in graph.items():\n    for t in links:\n        G.add_edge(node, t)\n\nnx_pr = nx.pagerank(G, alpha=0.85)\n\nprint(\"PageRank NetworkX:\")\nfor node, pr in nx_pr.items():\n    print(node, round(pr, 4))\n</code></pre>"},{"location":"pagerank/main/#interpretacao","title":"Interpreta\u00e7\u00e3o","text":"<p>Os resultados de PageRank Convergido e PageRank NetworkX s\u00e3o id\u00eanticos, o que confirma a correta implementa\u00e7\u00e3o e converg\u00eancia do algoritmo manual em rela\u00e7\u00e3o \u00e0 biblioteca padr\u00e3o (NetworkX).</p> <p>O valor de PageRank de um n\u00f3 pode ser interpretado como a probabilidade de um \"navegador aleat\u00f3rio\" (o modelo subjacente ao algoritmo) estar naquele n\u00f3 em um determinado momento. Quanto maior o valor, mais importante \u00e9 o n\u00f3 dentro da estrutura do grafo.</p>"},{"location":"pagerank/main/#quais-sao-os-nos-mais-importantes","title":"Quais s\u00e3o os n\u00f3s mais importantes?","text":"<ul> <li> <p>N\u00f3 C: O Mais Importante </p> <ul> <li>PageRank: 0.4292 (Mais Alto)</li> <li>O n\u00f3 C possui o maior PageRank, sendo considerado o mais importante da rede.Justificativa no Grafo: O n\u00f3 C recebe links de todos os outros tr\u00eas n\u00f3s (A -&gt; C, B -&gt; C, D -&gt; C). Mesmo que os n\u00f3s A e D recebam a mesma quantidade de PageRank de C, a contribui\u00e7\u00e3o de A, B e D para C o torna o centro de influ\u00eancia da rede.</li> </ul> </li> <li> <p>N\u00f3s A e D: Import\u00e2ncia Intermedi\u00e1ria/Igual</p> <ul> <li>PageRank: 0.2199 (Id\u00eantico)</li> <li>Os n\u00f3s A e D t\u00eam a mesma import\u00e2ncia no grafo, ocupando a segunda posi\u00e7\u00e3o.Justificativa no Grafo: Ambos recebem PageRank apenas do n\u00f3 C (C -&gt; A e C -&gt; D). Como o n\u00f3 C divide seu PageRank uniformemente entre A e D, e n\u00e3o h\u00e1 outras fontes de entrada para A ou D, eles terminam com o mesmo valor de import\u00e2ncia.</li> </ul> </li> <li> <p>N\u00f3 B: O Menos Importante</p> <ul> <li>PageRank: 0.131 (Mais Baixo)</li> <li>O n\u00f3 B tem o valor de PageRank mais baixo.Justificativa no Grafo: O n\u00f3 B recebe links apenas do n\u00f3 A (A -&gt; B). Como A tamb\u00e9m aponta para C, o PageRank de A \u00e9 dividido. Al\u00e9m disso, B n\u00e3o recebe nenhuma outra fonte de entrada. A baixa entrada de PageRank faz com que ele seja o n\u00f3 com menor relev\u00e2ncia.</li> </ul> </li> </ul>"},{"location":"projeto/main/","title":"Main","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p> <p>RMSE: 19847.42 R\u00b2: 0.860  2025-12-06T01:51:43.711482 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ </p>"},{"location":"random-forest/main/","title":"Random-Forest","text":""},{"location":"random-forest/main/#projeto-classificacao-de-sobreviventes-do-titanic-com-random-forest","title":"Projeto: Classifica\u00e7\u00e3o de Sobreviventes do Titanic com Random Forest","text":""},{"location":"random-forest/main/#dados-utilizados","title":"Dados utilizados","text":"<p>Dataset - Kaggle</p> PassengerIdSurvivedPclassSexAgeSibSpParchTicketFareEmbarked <p>Tipo: num\u00e9rica discreta O que \u00e9: identificador \u00fanico do passageiro. Para que serve: n\u00e3o cont\u00e9m informa\u00e7\u00e3o \u00fatil para predi\u00e7\u00e3o. A\u00e7\u00e3o necess\u00e1ria: remover do modelo.</p> 2025-12-06T01:51:44.075257 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica bin\u00e1ria (target) O que \u00e9: 1 = sobreviveu; 0 = n\u00e3o sobreviveu. Para que serve: vari\u00e1vel dependente a ser prevista. A\u00e7\u00e3o necess\u00e1ria: checar balanceamento (a classe 0 \u00e9 ligeiramente maior).</p> 2025-12-06T01:51:44.164527 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica ordinal O que \u00e9: classe socioecon\u00f4mica do ticket (1\u00aa, 2\u00aa, 3\u00aa). Para que serve: proxy de condi\u00e7\u00e3o financeira/social que influencia sobreviv\u00eancia. A\u00e7\u00e3o necess\u00e1ria: manter como categ\u00f3rica ordinal; verificar distribui\u00e7\u00e3o nas classes.</p> 2025-12-06T01:51:44.226269 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica bin\u00e1ria Oque \u00e9: sexo biol\u00f3gico do passageiro (male/female). Para que serve: uma das vari\u00e1veis mais importantes na sobreviv\u00eancia. A\u00e7\u00e3o necess\u00e1ria: codificar para dummy (female \u2192 1, male \u2192 0).</p> 2025-12-06T01:51:44.289321 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica cont\u00ednua O que \u00e9: idade em anos. Para que serve: importante para separar grupos vulner\u00e1veis (crian\u00e7as, adultos). A\u00e7\u00e3o necess\u00e1ria: 177 valores ausentes \u2192 imputar (m\u00e9dia/mediana ou por t\u00edtulo extra\u00eddo do Name).</p> 2025-12-06T01:51:44.361062 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica discreta O que \u00e9: n\u00famero de irm\u00e3os/c\u00f4njuges a bordo. Para que serve: indica tamanho do grupo familiar; pode influenciar sobreviv\u00eancia. A\u00e7\u00e3o necess\u00e1ria: manter; poss\u00edvel normalizar ou agrupar faixas.</p> 2025-12-06T01:51:44.457464 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica discreta O que \u00e9: n\u00famero de pais/filhos a bordo. Para que serve: outro indicador do grupo familiar. A\u00e7\u00e3o necess\u00e1ria: manter; poss\u00edvel criar \u201cFamilySize = SibSp + Parch + 1\u201d.</p> 2025-12-06T01:51:44.549410 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica (texto) O que \u00e9: n\u00famero/c\u00f3digo do ticket. Para que serve: pouco \u00fatil originalmente; pode ajudar se agrupado por prefixos. A\u00e7\u00e3o necess\u00e1ria: normalmente remover.</p> 2025-12-06T01:51:44.636011 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica cont\u00ednua O que \u00e9: tarifa paga pelo ticket. Para que serve: rela\u00e7\u00e3o com classe social; boa vari\u00e1vel preditiva. A\u00e7\u00e3o necess\u00e1ria: checar outliers; poss\u00edvel normaliza\u00e7\u00e3o logar\u00edtmica.</p> 2025-12-06T01:51:44.736592 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica nominal O que \u00e9: porto de embarque (C, Q, S). Para que serve: pode refletir diferen\u00e7as sociais/regionais. A\u00e7\u00e3o necess\u00e1ria: imputar os 2 valores ausentes; criar dummies.</p> 2025-12-06T01:51:44.819808 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/"},{"location":"random-forest/main/#random-forest","title":"Random Forest","text":"<p>O objetivo deste documento \u00e9 explicar o funcionamento te\u00f3rico do algoritmo Random Forest, relacionando-o com a implementa\u00e7\u00e3o pr\u00e1tica aplicada ao Titanic Dataset (Kaggle).</p>"},{"location":"random-forest/main/#o-que-e-random-forest-explicacao-teorica","title":"O que \u00e9 Random Forest? (Explica\u00e7\u00e3o Te\u00f3rica)","text":"<p>O Random Forest \u00e9 um algoritmo de aprendizado supervisionado do tipo ensemble, baseado em v\u00e1rias \u00e1rvores de decis\u00e3o.</p> <p>Ele funciona criando m\u00faltiplas \u00e1rvores de decis\u00e3o independentes e combinando seus resultados por meio de:</p> <ul> <li>Voting (para classifica\u00e7\u00e3o)</li> <li>Averaging (para regress\u00e3o)</li> </ul>"},{"location":"random-forest/main/#vantagens-do-random-forest","title":"Vantagens do Random Forest","text":"<ul> <li>Reduz o risco de overfitting comparado a uma \u00fanica \u00e1rvore.</li> <li>Funciona bem com dados num\u00e9ricos e categ\u00f3ricos.</li> <li>Tolera dados ruidosos.</li> <li>N\u00e3o exige normaliza\u00e7\u00e3o ou padroniza\u00e7\u00e3o.</li> <li>Mede automaticamente a import\u00e2ncia das vari\u00e1veis.</li> </ul>"},{"location":"random-forest/main/#principais-conceitos-que-fazem-o-random-forest-funcionar","title":"Principais conceitos que fazem o Random Forest funcionar","text":""},{"location":"random-forest/main/#bootstrap-aggregation-bagging","title":"Bootstrap Aggregation (Bagging)","text":"<p>Cada \u00e1rvore \u00e9 treinada em um subconjunto amostrado com reposi\u00e7\u00e3o do dataset original.</p> <p>Isso cria diversidade entre as \u00e1rvores.</p>"},{"location":"random-forest/main/#selecao-aleatoria-de-atributos","title":"Sele\u00e7\u00e3o aleat\u00f3ria de atributos","text":"<p>Ao construir cada divis\u00e3o na \u00e1rvore, o algoritmo considera apenas um subconjunto aleat\u00f3rio de vari\u00e1veis.</p> <p>Isso evita que todas as \u00e1rvores sejam iguais \u279c aumenta a generaliza\u00e7\u00e3o.</p>"},{"location":"random-forest/main/#out-of-bag-score-oob","title":"Out-of-Bag Score (OOB)","text":"<p>Como parte dos dados n\u00e3o entra na amostra do bootstrap, eles s\u00e3o usados como valida\u00e7\u00e3o interna, dispensando valida\u00e7\u00e3o cruzada.</p>"},{"location":"random-forest/main/#exploracao-dos-dados-aplicacao-pratica","title":"Explora\u00e7\u00e3o dos Dados (Aplica\u00e7\u00e3o Pr\u00e1tica)","text":""},{"location":"random-forest/main/#variaveis-principais-analisadas","title":"Vari\u00e1veis principais analisadas:","text":"<ul> <li><code>Pclass</code> </li> <li><code>Sex</code></li> <li><code>Age</code></li> <li><code>SibSp</code></li> <li><code>Parch</code></li> <li><code>Fare</code></li> <li><code>Embarked</code></li> <li><code>Survived</code> (vari\u00e1vel alvo)</li> </ul>"},{"location":"random-forest/main/#pre-processamento-pratica-explicacao-teorica","title":"Pr\u00e9-processamento (Pr\u00e1tica + Explica\u00e7\u00e3o Te\u00f3rica)","text":"<p>O Random Forest n\u00e3o exige padroniza\u00e7\u00e3o, por\u00e9m exige:</p> <pre><code>- Tratamento de valores ausentes\n- Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas (Aplicado One-Hot Encoding)\n</code></pre>"},{"location":"random-forest/main/#codigo-exemplo-e-resultado","title":"C\u00f3digo Exemplo e resultado","text":"outputcode <p>Acur\u00e1cia: 0.8044692737430168 OOB Score: 0.7935393258426966  Relat\u00f3rio de Classifica\u00e7\u00e3o:                precision    recall  f1-score   support             0       0.82      0.86      0.84       105            1       0.78      0.73      0.76        74      accuracy                           0.80       179    macro avg       0.80      0.79      0.80       179 weighted avg       0.80      0.80      0.80       179   Import\u00e2ncia das vari\u00e1veis:  Fare          0.274170 Sex_male      0.267026 Age           0.255208 Pclass        0.082134 SibSp         0.049823 Parch         0.037489 Embarked_S    0.023333 Embarked_Q    0.010816 dtype: float64 </p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport kagglehub\nimport pandas as pd\nimport os\n\n# 1. Baixar e carregar os dados\npath = kagglehub.dataset_download(\"yasserh/titanic-dataset\")\nfile_path = os.path.join(path, \"Titanic-Dataset.csv\")\ndf = pd.read_csv(file_path)\n\n# 2. Selecionar colunas relevantes\nfeatures = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Survived']].copy()\n\n# 3. Tratar valores ausentes\nfeatures['Age'].fillna(features['Age'].mean(), inplace=True)\nfeatures['Embarked'].fillna(features['Embarked'].mode()[0], inplace=True)\n\n# 4. Converter vari\u00e1veis categ\u00f3ricas em num\u00e9ricas (One-Hot Encoding)\nfeatures = pd.get_dummies(features, columns=['Sex', 'Embarked'], drop_first=True)\n\n# 5. Definir X (entradas) e y (alvo)\nX = features.drop('Survived', axis=1)\ny = features['Survived']\n\n# 6. Dividir treino/teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 7. Criar modelo Random Forest\nrf = RandomForestClassifier(\n    n_estimators=100,        # n\u00famero de \u00e1rvores\n    max_depth=None,          # profundidade ilimitada\n    max_features='sqrt',     # n\u00ba de features consideradas em cada split\n    oob_score=True,          # valida\u00e7\u00e3o out-of-bag\n    random_state=42\n)\n\n# 8. Treinar modelo\nrf.fit(X_train, y_train)\n\n# 9. Avaliar modelo\ny_pred = rf.predict(X_test)\nprint(\"Acur\u00e1cia:\", accuracy_score(y_test, y_pred))\nprint(\"OOB Score:\", rf.oob_score_)\nprint(\"\\nRelat\u00f3rio de Classifica\u00e7\u00e3o:\\n\", classification_report(y_test, y_pred))\n\n# 10. Import\u00e2ncia das vari\u00e1veis\nimportances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\nprint(\"\\nImport\u00e2ncia das vari\u00e1veis:\\n\", importances)\n</code></pre>"},{"location":"random-forest/main/#divisao-dos-dados","title":"Divis\u00e3o dos Dados","text":"<pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n</code></pre> <ul> <li>80% treino</li> <li>20% teste</li> </ul>"},{"location":"random-forest/main/#treinamento-do-modelo-pratica-explicacao-teorica","title":"Treinamento do Modelo (Pr\u00e1tica + Explica\u00e7\u00e3o Te\u00f3rica)","text":"<p>Foi utilizado o RandomForestClassifier, com os seguintes hiperpar\u00e2metros:</p>"},{"location":"random-forest/main/#parametros-utilizados","title":"Par\u00e2metros utilizados","text":"<ul> <li><code>n_estimators=100</code> \u2192 100 \u00e1rvores</li> <li><code>max_depth=None</code> \u2192 \u00e1rvores podem crescer livremente</li> <li><code>max_features='sqrt'</code> \u2192 raiz quadrada do n\u00famero de vari\u00e1veis</li> <li><code>oob_score=True</code> \u2192 ativa\u00e7\u00e3o da valida\u00e7\u00e3o Out-of-Bag</li> <li><code>random_state=42</code> \u2192 garante reprodutibilidade</li> </ul>"},{"location":"random-forest/main/#por-que-max_featuressqrt","title":"Por que <code>max_features='sqrt'</code>?","text":"<p>Essa \u00e9 uma estrat\u00e9gia padr\u00e3o para: - aumentar a diversidade entre as \u00e1rvores - reduzir correla\u00e7\u00e3o entre elas - melhorar a generaliza\u00e7\u00e3o</p>"},{"location":"random-forest/main/#avaliacao-do-modelo","title":"Avalia\u00e7\u00e3o do Modelo","text":""},{"location":"random-forest/main/#metricas-obtidas","title":"M\u00e9tricas obtidas","text":"<pre><code>Acur\u00e1cia: 0.804\nOOB Score: 0.794\n</code></pre> <p>O OOB Score pr\u00f3ximo da acur\u00e1cia indica que: - o modelo n\u00e3o est\u00e1 sofrendo overfitting - a validade interna \u00e9 consistente</p>"},{"location":"random-forest/main/#relatorio-de-classificacao","title":"Relat\u00f3rio de classifica\u00e7\u00e3o","text":"<pre><code>              precision    recall  f1-score   support\n           0       0.82      0.86      0.84       105\n           1       0.78      0.73      0.76        74\n    accuracy                           0.80       179\n   macro avg       0.80      0.79      0.80       179\nweighted avg       0.80      0.80      0.80       179\n</code></pre> <p>As classes est\u00e3o relativamente equilibradas nos resultados.</p>"},{"location":"random-forest/main/#importancia-das-variaveis-teoria-pratica","title":"Import\u00e2ncia das Vari\u00e1veis (Teoria + Pr\u00e1tica)","text":"<p>O Random Forest mede import\u00e2ncia das vari\u00e1veis com base em: - redu\u00e7\u00e3o m\u00e9dia da impureza (Gini ou Entropia) - ou permuta\u00e7\u00e3o de vari\u00e1veis (quando configurado)</p>"},{"location":"random-forest/main/#importancias-obtidas","title":"Import\u00e2ncias obtidas","text":"Vari\u00e1vel Import\u00e2ncia Fare 0.274 Sex_male 0.267 Age 0.255 Pclass 0.082 SibSp 0.050 Parch 0.037 Embarked_S 0.023 Embarked_Q 0.011"},{"location":"random-forest/main/#interpretacao","title":"Interpreta\u00e7\u00e3o","text":"<ul> <li>Fare (valor da passagem) \u2192 principal indicador   Passageiros de classes mais altas sobreviveram mais.</li> <li>Sex_male \u2192 homens tiveram menor chance de sobreviver.</li> <li>Age \u2192 crian\u00e7as tiveram prioridade em barcos.</li> <li>Outros atributos tiveram menor influ\u00eancia.</li> </ul>"},{"location":"random-forest/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O modelo apresentou: - Acur\u00e1cia \u2248 80% - Excelente estabilidade (OOB Score similar) - Boa capacidade de generaliza\u00e7\u00e3o</p>"},{"location":"random-forest/main/#conclusoes-importantes-sobre-o-titanic","title":"Conclus\u00f5es importantes sobre o Titanic","text":"<ul> <li>Mulheres e crian\u00e7as tiveram maior probabilidade de sobreviver.</li> <li>Passagens mais caras \u2192 maior taxa de sobreviv\u00eancia.</li> <li>Vari\u00e1veis como <code>SibSp</code>, <code>Embarked</code> e <code>Parch</code> tiveram menor influ\u00eancia no modelo.</li> </ul>"},{"location":"svm/main/","title":"svm","text":""},{"location":"svm/main/#o-dataset","title":"O dataset","text":"<p>Dataset - Kaggle</p> PassengerIdSurvivedPclassSexAgeSibSpParchTicketFareEmbarked <p>Tipo: num\u00e9rica discreta O que \u00e9: identificador \u00fanico do passageiro. Para que serve: n\u00e3o cont\u00e9m informa\u00e7\u00e3o \u00fatil para predi\u00e7\u00e3o. A\u00e7\u00e3o necess\u00e1ria: remover do modelo.</p> 2025-12-06T01:51:45.398642 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica bin\u00e1ria (target) O que \u00e9: 1 = sobreviveu; 0 = n\u00e3o sobreviveu. Para que serve: vari\u00e1vel dependente a ser prevista. A\u00e7\u00e3o necess\u00e1ria: checar balanceamento (a classe 0 \u00e9 ligeiramente maior).</p> 2025-12-06T01:51:45.487477 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica ordinal O que \u00e9: classe socioecon\u00f4mica do ticket (1\u00aa, 2\u00aa, 3\u00aa). Para que serve: proxy de condi\u00e7\u00e3o financeira/social que influencia sobreviv\u00eancia. A\u00e7\u00e3o necess\u00e1ria: manter como categ\u00f3rica ordinal; verificar distribui\u00e7\u00e3o nas classes.</p> 2025-12-06T01:51:45.550406 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica bin\u00e1ria Oque \u00e9: sexo biol\u00f3gico do passageiro (male/female). Para que serve: uma das vari\u00e1veis mais importantes na sobreviv\u00eancia. A\u00e7\u00e3o necess\u00e1ria: codificar para dummy (female \u2192 1, male \u2192 0).</p> 2025-12-06T01:51:45.612596 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica cont\u00ednua O que \u00e9: idade em anos. Para que serve: importante para separar grupos vulner\u00e1veis (crian\u00e7as, adultos). A\u00e7\u00e3o necess\u00e1ria: 177 valores ausentes \u2192 imputar (m\u00e9dia/mediana ou por t\u00edtulo extra\u00eddo do Name).</p> 2025-12-06T01:51:45.685871 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica discreta O que \u00e9: n\u00famero de irm\u00e3os/c\u00f4njuges a bordo. Para que serve: indica tamanho do grupo familiar; pode influenciar sobreviv\u00eancia. A\u00e7\u00e3o necess\u00e1ria: manter; poss\u00edvel normalizar ou agrupar faixas.</p> 2025-12-06T01:51:45.781921 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica discreta O que \u00e9: n\u00famero de pais/filhos a bordo. Para que serve: outro indicador do grupo familiar. A\u00e7\u00e3o necess\u00e1ria: manter; poss\u00edvel criar \u201cFamilySize = SibSp + Parch + 1\u201d.</p> 2025-12-06T01:51:45.873650 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica (texto) O que \u00e9: n\u00famero/c\u00f3digo do ticket. Para que serve: pouco \u00fatil originalmente; pode ajudar se agrupado por prefixos. A\u00e7\u00e3o necess\u00e1ria: normalmente remover.</p> 2025-12-06T01:51:45.960399 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: num\u00e9rica cont\u00ednua O que \u00e9: tarifa paga pelo ticket. Para que serve: rela\u00e7\u00e3o com classe social; boa vari\u00e1vel preditiva. A\u00e7\u00e3o necess\u00e1ria: checar outliers; poss\u00edvel normaliza\u00e7\u00e3o logar\u00edtmica.</p> 2025-12-06T01:51:46.060748 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Tipo: categ\u00f3rica nominal O que \u00e9: porto de embarque (C, Q, S). Para que serve: pode refletir diferen\u00e7as sociais/regionais. A\u00e7\u00e3o necess\u00e1ria: imputar os 2 valores ausentes; criar dummies.</p> 2025-12-06T01:51:46.293193 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/"},{"location":"svm/main/#support-vector-machine-svm","title":"Support Vector Machine (SVM)","text":"<p>Support Vector Machine</p> <p>O Support Vector Machine (SVM) \u00e9 um algoritmo supervisionado amplamente utilizado para classifica\u00e7\u00e3o (e tamb\u00e9m dispon\u00edvel em variantes de regress\u00e3o, como SVR). O objetivo do SVM \u00e9 encontrar a melhor fronteira (linha, plano ou hiperplano) que separa classes no espa\u00e7o de features.</p>"},{"location":"svm/main/#conceito-basico","title":"Conceito b\u00e1sico","text":"<p>Em problemas de classifica\u00e7\u00e3o bin\u00e1ria (por exemplo, \"sobreviveu\" vs \"n\u00e3o sobreviveu\"), o SVM busca o hiperplano que maximize a margem \u2014 ou seja, a dist\u00e2ncia entre o hiperplano e os pontos mais pr\u00f3ximos de cada classe (os vetores de suporte).</p> <p>Quanto maior a margem, mais robusta tende a ser a separa\u00e7\u00e3o entre classes.</p>"},{"location":"svm/main/#intuicao-visual","title":"Intui\u00e7\u00e3o visual","text":"<p>Imagine duas nuvens de pontos. Embora v\u00e1rias linhas possam separar as nuvens, o SVM seleciona a que:</p> <ul> <li>maximiza a margem;</li> <li>est\u00e1 o mais distante poss\u00edvel dos pontos das duas classes;</li> <li>\u00e9 definida por poucos pontos cr\u00edticos (os vetores de suporte).</li> </ul>"},{"location":"svm/main/#kernel-trick","title":"Kernel Trick","text":"<p>Quando os dados n\u00e3o s\u00e3o linearmente separ\u00e1veis no espa\u00e7o original, o SVM pode aplicar uma transforma\u00e7\u00e3o (kernel) que projeta os dados para um espa\u00e7o de maior dimens\u00e3o onde a separa\u00e7\u00e3o \u00e9 poss\u00edvel.</p>"},{"location":"svm/main/#principais-kernels","title":"Principais kernels","text":"<ul> <li>Linear: separa\u00e7\u00e3o por uma linha/hiperplano;</li> <li>Polinomial: permite curvas polinomiais (grau 2, 3, ...);</li> <li>RBF (Radial Basis Function): produz fronteiras complexas; \u00e9 o mais usado na pr\u00e1tica;</li> <li>Sigmoid: comportamento similar a redes neurais simples.</li> </ul>"},{"location":"svm/main/#script-e-resultado","title":"Script e resultado","text":"outputcode 2025-12-06T01:51:46.821016 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom io import StringIO\nfrom sklearn.preprocessing import StandardScaler\n\n# ============================\n# Carregar dataset\n# ============================\ndf = pd.read_csv(\"Titanic-Dataset.csv\")\n\n# Selecionar apenas as colunas necess\u00e1rias\ndf = df[['Survived', 'Age', 'Fare', 'Sex']]\n\n# Converter vari\u00e1vel categ\u00f3rica\ndf['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n\n# Remover valores faltantes\ndf = df.dropna()\n\n# ============================\n# Sele\u00e7\u00e3o de Features para plot 2D\n# ============================\n# Usaremos apenas Age e Fare para plotar a fronteira\nX = df[['Age', 'Fare']].values\ny = df['Survived'].values\n\n# Padronizar\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# ============================\n# Plot\n# ============================\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 6))\n\nkernels = {\n    'linear': ax1,\n    'sigmoid': ax2,\n    'poly': ax3,\n    'rbf': ax4\n}\n\nfor k, ax in kernels.items():\n    svm = SVC(kernel=k, C=1)\n    svm.fit(X, y)\n\n    DecisionBoundaryDisplay.from_estimator(\n        svm,\n        X,\n        response_method=\"predict\",\n        alpha=0.8,\n        cmap=\"Pastel1\",\n        ax=ax\n    )\n\n    ax.scatter(\n        X[:, 0], X[:, 1],\n        c=y,\n        s=20,\n        edgecolors=\"k\"\n    )\n\n    ax.set_title(f\"SVM Kernel: {k}\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n# Salvar como SVG em buffer\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"svm/main/#o-script-explicado","title":"O script explicado","text":"<p>Esta se\u00e7\u00e3o descreve passo a passo o script utilizado no exemplo (arquivo <code>docs/svm/svm.py</code>). Trechos de c\u00f3digo relevantes foram convertidos em blocos Python para facilitar a leitura e a execu\u00e7\u00e3o em MkDocs.</p>"},{"location":"svm/main/#1-carregamento-e-preparacao-dos-dados","title":"1. Carregamento e prepara\u00e7\u00e3o dos dados","text":"<pre><code>df = pd.read_csv(\"Titanic-Dataset.csv\")\ndf = df[['Survived', 'Age', 'Fare', 'Sex']]\ndf['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\ndf = df.dropna()\n</code></pre> <ul> <li>Carrega o dataset do Titanic;</li> <li>Seleciona vari\u00e1veis relevantes;</li> <li>Converte <code>Sex</code> para valores num\u00e9ricos;</li> <li>Remove valores faltantes (necess\u00e1rio para treinar o modelo).</li> </ul>"},{"location":"svm/main/#2-selecao-de-features-para-visualizacao","title":"2. Sele\u00e7\u00e3o de features para visualiza\u00e7\u00e3o","text":"<pre><code>X = df[['Age', 'Fare']].values\ny = df['Survived'].values\n</code></pre> <p>Nesta demonstra\u00e7\u00e3o usamos apenas <code>Age</code> e <code>Fare</code> para manter os dados em 2D (necess\u00e1rio para plotar as fronteiras de decis\u00e3o com <code>DecisionBoundaryDisplay</code>).</p>"},{"location":"svm/main/#3-padronizacao","title":"3. Padroniza\u00e7\u00e3o","text":"<pre><code>scaler = StandardScaler()\nX = scaler.fit_transform(X)\n</code></pre> <p>O SVM costuma se beneficiar de dados escalonados, especialmente para kernels como RBF e polinomial.</p>"},{"location":"svm/main/#4-treinamento-com-multiplos-kernels","title":"4. Treinamento com m\u00faltiplos kernels","text":"<pre><code>kernels = {\n    'linear': ax1,\n    'sigmoid': ax2,\n    'poly': ax3,\n    'rbf': ax4\n}\n\nfor k, ax in kernels.items():\n    svm = SVC(kernel=k, C=1)\n    svm.fit(X, y)\n</code></pre> <p>O script treina um modelo SVM para cada kernel (<code>linear</code>, <code>sigmoid</code>, <code>poly</code>, <code>rbf</code>) para comparar as fronteiras de decis\u00e3o.</p>"},{"location":"svm/main/#5-plotando-a-fronteira-de-decisao","title":"5. Plotando a fronteira de decis\u00e3o","text":"<pre><code>DecisionBoundaryDisplay.from_estimator(\n    svm,\n    X,\n    response_method=\"predict\",\n    alpha=0.8,\n    cmap=\"Pastel1\",\n    ax=ax\n)\n</code></pre> <p>Essa fun\u00e7\u00e3o desenha a fronteira de decis\u00e3o aprendida pelo SVM. A forma da fronteira varia conforme o kernel:</p> <ul> <li>linear: linha reta;</li> <li>poly: curvas suaves;</li> <li>rbf: fronteiras detalhadas e n\u00e3o lineares;</li> <li>sigmoid: separa\u00e7\u00f5es parecidas com fun\u00e7\u00f5es tipo rede neural.</li> </ul>"},{"location":"svm/main/#6-plotando-os-pontos-de-dados","title":"6. Plotando os pontos de dados","text":"<pre><code>ax.scatter(\n    X[:, 0], X[:, 1],\n    c=y,\n    s=20, edgecolors=\"k\"\n)\n</code></pre> <p>Isso plota os pontos reais do dataset sobre a superf\u00edcie de decis\u00e3o (1 = sobrevivente, 0 = n\u00e3o sobrevivente).</p>"},{"location":"svm/main/#7-salvando-a-figura-como-svg-em-buffer","title":"7. Salvando a figura como SVG em buffer","text":"<pre><code>buffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</code></pre> <p>Em vez de mostrar a imagem interativamente, o script pode retornar o conte\u00fado SVG \u2014 \u00fatil para inclus\u00e3o em p\u00e1ginas HTML ou MkDocs.</p>"},{"location":"svm/main/#o-que-a-visualizacao-permite-analisar","title":"O que a visualiza\u00e7\u00e3o permite analisar","text":"<ul> <li>Regi\u00f5es onde o modelo prev\u00ea sobreviv\u00eancia vs. n\u00e3o sobreviv\u00eancia;</li> <li>Complexidade das fronteiras de decis\u00e3o e flexibilidade do modelo;</li> <li>Compara\u00e7\u00e3o direta entre kernels (rigidez vs. flexibilidade).</li> </ul> <p>Exemplos:</p> <ul> <li>Linear: fronteira reta;</li> <li>Poly: curvas mais suaves;</li> <li>RBF: fronteiras com muitos detalhes;</li> <li>Sigmoid: comportamento intermedi\u00e1rio.</li> </ul>"},{"location":"svm/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O SVM \u00e9 um modelo poderoso que:</p> <ul> <li>Busca a melhor separa\u00e7\u00e3o entre classes;</li> <li>Utiliza vetores de suporte como pontos cr\u00edticos;</li> <li>Pode gerar fronteiras lineares ou altamente n\u00e3o lineares via kernels;</li> <li>Se beneficia de escalonamento dos dados.</li> </ul> <p>O exemplo pr\u00e1tico demonstra como diferentes kernels afetam a decis\u00e3o do modelo no problema do Titanic e ajuda a entender seu comportamento.</p>"}]}